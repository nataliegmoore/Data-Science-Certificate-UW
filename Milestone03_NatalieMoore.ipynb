{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Milestone03-NatalieMoore.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10X-EsnUMfIi",
        "colab_type": "text"
      },
      "source": [
        "# Begin Milestone01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcD9UyzHMfIl",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "My goal with this large dataset was to select a subset of features that would best be able to model the relationship between all the features and their labels (-1 or 1). After attempting to find the best feature selection, it looks at first like Embedded method features have the highest accuracy with this senario. However, it's a bit of a red flag that most of the model methods have the highest accuracy with the Embedded features. The Embedded features may be overfitting the dataset. \n",
        "\n",
        "Instead, I've decided to use the Wrapper method features because they scored the 2nd highest in accuracy mean accross all feature selection methods and model types.\n",
        "\n",
        "What I did:\n",
        "- loaded the two datasets\n",
        "- removed outliers\n",
        "- changed date column to be datetime\n",
        "- merged the dataframes\n",
        "- imputed missing values (NaNs) with the most frequent value\n",
        "- created a correlation matrix and visualized it with a heatmap\n",
        "- handled the class imbalance problem using SMOTE\n",
        "- used Filter, Wrapper, and Embedded methods to select a subset of features for my model\n",
        "- tried out 6 different model types with all three filter selections and plotted accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmh1LGiPMfIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import matlib\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics, svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import SMOTE \n",
        "import datetime as dt\n",
        "import scipy.stats as ss\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "import statsmodels.stats.power as smsp\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import graphviz\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "#matplotlib formatting that I like\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYCyIqjHMfI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading and reading the datasets\n",
        "filename_data = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\n",
        "filename_labels = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\n",
        "\n",
        "df1 = pd.read_csv(filename_data, sep = ' ', header = None)\n",
        "df2 = pd.read_csv(filename_labels, sep = ' ', header = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2clDLXXMfJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#finding and removing any outliers (points above the upper and below the low percentile)\n",
        "#only for columns 0 - 589 columns 590 and 591 are classifiers and datetime\n",
        "def remove_outlier(df):\n",
        "    low = .05\n",
        "    high = .95\n",
        "    quant_df = df.quantile([low, high])\n",
        "    good_points = pd.DataFrame()\n",
        "    for y in range(0,589):\n",
        "        not_outlier = df[y][(df[y] > quant_df.loc[low, y]) & (df[y] < quant_df.loc[high, y])]\n",
        "        if len(not_outlier) > 1299:\n",
        "            good_points[y] = not_outlier\n",
        "        else:\n",
        "            good_points[y] = df[y]\n",
        "    return good_points\n",
        "\n",
        "df1_NoOutliers = remove_outlier(df1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3mzMJEPMfJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae9922be-02ae-4581-c9c1-8f88946af441"
      },
      "source": [
        "#need to change the collection dates (column 590) to datetime format!\n",
        "df2[1] = pd.to_datetime(df2[1], format='%d/%m/%Y %H:%M:%S')\n",
        "print(\"Min Date=%s, Max Date=%s\"%(min(df2[1]).strftime(\"%Y-%m-%d\"), \\\n",
        "                                          max(df2[1]).strftime(\"%Y-%m-%d\")))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min Date=2008-07-19, Max Date=2008-10-17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St2NYDhwMfJd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "a2f35dc3-34cc-4f69-d67e-39a8dae5b130"
      },
      "source": [
        "#merging the two datasets\n",
        "df1_NoOutliers[590], df1_NoOutliers[591] = df2[1], df2[0]\n",
        "diaper_df = df1_NoOutliers\n",
        "diaper_df = diaper_df.sort_values(by = 590) #sorted by datetime\n",
        "\n",
        "diaper_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "      <th>561</th>\n",
              "      <th>562</th>\n",
              "      <th>563</th>\n",
              "      <th>564</th>\n",
              "      <th>565</th>\n",
              "      <th>566</th>\n",
              "      <th>567</th>\n",
              "      <th>568</th>\n",
              "      <th>569</th>\n",
              "      <th>570</th>\n",
              "      <th>571</th>\n",
              "      <th>572</th>\n",
              "      <th>573</th>\n",
              "      <th>574</th>\n",
              "      <th>575</th>\n",
              "      <th>576</th>\n",
              "      <th>577</th>\n",
              "      <th>578</th>\n",
              "      <th>579</th>\n",
              "      <th>580</th>\n",
              "      <th>581</th>\n",
              "      <th>582</th>\n",
              "      <th>583</th>\n",
              "      <th>584</th>\n",
              "      <th>585</th>\n",
              "      <th>586</th>\n",
              "      <th>587</th>\n",
              "      <th>588</th>\n",
              "      <th>590</th>\n",
              "      <th>591</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3030.93</td>\n",
              "      <td>2564.00</td>\n",
              "      <td>2187.7333</td>\n",
              "      <td>1411.1265</td>\n",
              "      <td>1.3602</td>\n",
              "      <td>100.0</td>\n",
              "      <td>97.6133</td>\n",
              "      <td>0.1242</td>\n",
              "      <td>1.5005</td>\n",
              "      <td>0.0162</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>NaN</td>\n",
              "      <td>202.4396</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.9558</td>\n",
              "      <td>414.8710</td>\n",
              "      <td>10.0433</td>\n",
              "      <td>0.9680</td>\n",
              "      <td>192.3963</td>\n",
              "      <td>12.5190</td>\n",
              "      <td>1.4026</td>\n",
              "      <td>-5419.00</td>\n",
              "      <td>2916.50</td>\n",
              "      <td>-4043.75</td>\n",
              "      <td>751.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>64.2333</td>\n",
              "      <td>2.0222</td>\n",
              "      <td>0.1632</td>\n",
              "      <td>3.5191</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.5126</td>\n",
              "      <td>50.6170</td>\n",
              "      <td>64.2588</td>\n",
              "      <td>49.3830</td>\n",
              "      <td>66.3141</td>\n",
              "      <td>86.9555</td>\n",
              "      <td>117.5132</td>\n",
              "      <td>...</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.1827</td>\n",
              "      <td>5.7349</td>\n",
              "      <td>0.3363</td>\n",
              "      <td>39.8842</td>\n",
              "      <td>3.2687</td>\n",
              "      <td>1.0297</td>\n",
              "      <td>1.0344</td>\n",
              "      <td>0.4385</td>\n",
              "      <td>0.1039</td>\n",
              "      <td>42.3877</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>533.8500</td>\n",
              "      <td>2.1113</td>\n",
              "      <td>8.95</td>\n",
              "      <td>0.3157</td>\n",
              "      <td>3.0624</td>\n",
              "      <td>0.1026</td>\n",
              "      <td>1.6765</td>\n",
              "      <td>14.9509</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.5005</td>\n",
              "      <td>0.0118</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>2.3630</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2008-07-19 11:55:00</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3095.78</td>\n",
              "      <td>2465.14</td>\n",
              "      <td>2230.4222</td>\n",
              "      <td>1463.6606</td>\n",
              "      <td>0.8294</td>\n",
              "      <td>100.0</td>\n",
              "      <td>102.3433</td>\n",
              "      <td>0.1247</td>\n",
              "      <td>1.4966</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>-0.0148</td>\n",
              "      <td>0.9627</td>\n",
              "      <td>200.5470</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.1548</td>\n",
              "      <td>414.7347</td>\n",
              "      <td>9.2599</td>\n",
              "      <td>0.9701</td>\n",
              "      <td>191.2872</td>\n",
              "      <td>12.4608</td>\n",
              "      <td>1.3825</td>\n",
              "      <td>-5441.50</td>\n",
              "      <td>2604.25</td>\n",
              "      <td>-3498.75</td>\n",
              "      <td>-1640.25</td>\n",
              "      <td>1.2973</td>\n",
              "      <td>2.0143</td>\n",
              "      <td>7.3900</td>\n",
              "      <td>68.4222</td>\n",
              "      <td>2.2667</td>\n",
              "      <td>0.2102</td>\n",
              "      <td>3.4171</td>\n",
              "      <td>84.9052</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50.6596</td>\n",
              "      <td>64.2828</td>\n",
              "      <td>49.3404</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>118.1188</td>\n",
              "      <td>...</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.2829</td>\n",
              "      <td>7.1196</td>\n",
              "      <td>0.4989</td>\n",
              "      <td>53.1836</td>\n",
              "      <td>3.9139</td>\n",
              "      <td>1.7819</td>\n",
              "      <td>0.9634</td>\n",
              "      <td>0.1745</td>\n",
              "      <td>0.0375</td>\n",
              "      <td>18.1087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>535.0164</td>\n",
              "      <td>2.4335</td>\n",
              "      <td>5.92</td>\n",
              "      <td>0.2653</td>\n",
              "      <td>2.0111</td>\n",
              "      <td>0.0772</td>\n",
              "      <td>1.1065</td>\n",
              "      <td>10.9003</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>208.2045</td>\n",
              "      <td>0.5019</td>\n",
              "      <td>0.0223</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.4447</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>2008-07-19 12:32:00</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2932.61</td>\n",
              "      <td>2559.94</td>\n",
              "      <td>2186.4111</td>\n",
              "      <td>1698.0172</td>\n",
              "      <td>1.5102</td>\n",
              "      <td>100.0</td>\n",
              "      <td>95.4878</td>\n",
              "      <td>0.1241</td>\n",
              "      <td>1.4436</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.9615</td>\n",
              "      <td>202.0179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5157</td>\n",
              "      <td>416.7075</td>\n",
              "      <td>9.3144</td>\n",
              "      <td>0.9674</td>\n",
              "      <td>192.7035</td>\n",
              "      <td>12.5404</td>\n",
              "      <td>1.4123</td>\n",
              "      <td>-5447.75</td>\n",
              "      <td>2701.75</td>\n",
              "      <td>-4047.00</td>\n",
              "      <td>-1916.50</td>\n",
              "      <td>1.3122</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>67.1333</td>\n",
              "      <td>2.3333</td>\n",
              "      <td>0.1734</td>\n",
              "      <td>3.5986</td>\n",
              "      <td>84.7569</td>\n",
              "      <td>8.6590</td>\n",
              "      <td>50.1530</td>\n",
              "      <td>64.1114</td>\n",
              "      <td>49.8470</td>\n",
              "      <td>65.8389</td>\n",
              "      <td>NaN</td>\n",
              "      <td>118.6128</td>\n",
              "      <td>...</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.0857</td>\n",
              "      <td>7.1619</td>\n",
              "      <td>0.3752</td>\n",
              "      <td>23.0713</td>\n",
              "      <td>3.9306</td>\n",
              "      <td>1.1386</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.3718</td>\n",
              "      <td>0.1233</td>\n",
              "      <td>24.7524</td>\n",
              "      <td>267.064</td>\n",
              "      <td>0.9032</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.6219</td>\n",
              "      <td>0.4122</td>\n",
              "      <td>0.2562</td>\n",
              "      <td>0.4119</td>\n",
              "      <td>68.8489</td>\n",
              "      <td>535.0245</td>\n",
              "      <td>2.0293</td>\n",
              "      <td>11.21</td>\n",
              "      <td>0.1882</td>\n",
              "      <td>4.0923</td>\n",
              "      <td>0.0640</td>\n",
              "      <td>2.0952</td>\n",
              "      <td>9.2721</td>\n",
              "      <td>0.0584</td>\n",
              "      <td>0.0484</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>82.8602</td>\n",
              "      <td>0.4958</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>3.1745</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2008-07-19 13:17:00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2988.72</td>\n",
              "      <td>2479.90</td>\n",
              "      <td>2199.0333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.3204</td>\n",
              "      <td>100.0</td>\n",
              "      <td>104.2367</td>\n",
              "      <td>0.1217</td>\n",
              "      <td>1.4882</td>\n",
              "      <td>-0.0124</td>\n",
              "      <td>-0.0033</td>\n",
              "      <td>0.9629</td>\n",
              "      <td>201.8482</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.6052</td>\n",
              "      <td>422.2894</td>\n",
              "      <td>9.6924</td>\n",
              "      <td>0.9687</td>\n",
              "      <td>192.1557</td>\n",
              "      <td>12.4782</td>\n",
              "      <td>1.4011</td>\n",
              "      <td>-5468.25</td>\n",
              "      <td>2648.25</td>\n",
              "      <td>-4515.00</td>\n",
              "      <td>-1657.25</td>\n",
              "      <td>1.3137</td>\n",
              "      <td>2.0038</td>\n",
              "      <td>7.3145</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.6444</td>\n",
              "      <td>0.2071</td>\n",
              "      <td>3.3813</td>\n",
              "      <td>84.9105</td>\n",
              "      <td>8.6789</td>\n",
              "      <td>50.5100</td>\n",
              "      <td>64.1125</td>\n",
              "      <td>49.4900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>86.6867</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>39.33</td>\n",
              "      <td>0.6812</td>\n",
              "      <td>56.9303</td>\n",
              "      <td>17.4781</td>\n",
              "      <td>161.4081</td>\n",
              "      <td>35.3198</td>\n",
              "      <td>54.2917</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>62.7572</td>\n",
              "      <td>268.228</td>\n",
              "      <td>0.6511</td>\n",
              "      <td>7.32</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>3.5611</td>\n",
              "      <td>0.0670</td>\n",
              "      <td>2.7290</td>\n",
              "      <td>25.0363</td>\n",
              "      <td>530.5682</td>\n",
              "      <td>2.0253</td>\n",
              "      <td>9.33</td>\n",
              "      <td>0.1738</td>\n",
              "      <td>2.8971</td>\n",
              "      <td>0.0525</td>\n",
              "      <td>1.7585</td>\n",
              "      <td>8.5831</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>73.8432</td>\n",
              "      <td>0.4990</td>\n",
              "      <td>0.0103</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0544</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>2008-07-19 14:43:00</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3032.24</td>\n",
              "      <td>2502.87</td>\n",
              "      <td>2233.3667</td>\n",
              "      <td>1326.5200</td>\n",
              "      <td>1.5334</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.3967</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>1.5031</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>-0.0072</td>\n",
              "      <td>0.9569</td>\n",
              "      <td>201.9424</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.5661</td>\n",
              "      <td>420.5925</td>\n",
              "      <td>10.3387</td>\n",
              "      <td>0.9735</td>\n",
              "      <td>191.6037</td>\n",
              "      <td>12.4735</td>\n",
              "      <td>1.3888</td>\n",
              "      <td>-5476.25</td>\n",
              "      <td>2635.25</td>\n",
              "      <td>-3987.50</td>\n",
              "      <td>117.00</td>\n",
              "      <td>1.2887</td>\n",
              "      <td>1.9912</td>\n",
              "      <td>7.2748</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.2728</td>\n",
              "      <td>86.3269</td>\n",
              "      <td>8.7677</td>\n",
              "      <td>50.2480</td>\n",
              "      <td>64.1511</td>\n",
              "      <td>49.7520</td>\n",
              "      <td>66.1542</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.4287</td>\n",
              "      <td>9.7608</td>\n",
              "      <td>0.8311</td>\n",
              "      <td>70.9706</td>\n",
              "      <td>4.9086</td>\n",
              "      <td>2.5014</td>\n",
              "      <td>0.9778</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.0461</td>\n",
              "      <td>22.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>532.0155</td>\n",
              "      <td>2.0275</td>\n",
              "      <td>8.83</td>\n",
              "      <td>0.2224</td>\n",
              "      <td>3.1776</td>\n",
              "      <td>0.0706</td>\n",
              "      <td>1.6597</td>\n",
              "      <td>10.9698</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0149</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>2008-07-19 15:22:00</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 591 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0        1          2    ...     588                 590  591\n",
              "0  3030.93  2564.00  2187.7333  ...     NaN 2008-07-19 11:55:00   -1\n",
              "1  3095.78  2465.14  2230.4222  ...  0.0060 2008-07-19 12:32:00   -1\n",
              "2  2932.61  2559.94  2186.4111  ...     NaN 2008-07-19 13:17:00    1\n",
              "3  2988.72  2479.90  2199.0333  ...  0.0044 2008-07-19 14:43:00   -1\n",
              "4  3032.24  2502.87  2233.3667  ...  0.0044 2008-07-19 15:22:00   -1\n",
              "\n",
              "[5 rows x 591 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ7uvwLLMfJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4f9fdf83-3cbe-4472-ea8a-88269dda0f41"
      },
      "source": [
        "#imputing missing values with the most frequent value in its column\n",
        "for i in diaper_df.columns:\n",
        "    mean_value=diaper_df[i].mode()[0]\n",
        "    diaper_df[i]=diaper_df[i].fillna(mean_value)\n",
        "\n",
        "#checking to make sure no more NaNs exist in diaper_df\n",
        "print(\"NaN count in diaper_df: \", diaper_df.isnull().sum().sum())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NaN count in diaper_df:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LO7n89K8MfJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "63f8bbb6-d10a-4d6f-a85d-61bd72a2ffc0"
      },
      "source": [
        "#attempting to visualize all the data at once\n",
        "\n",
        "#creating a correlation matrix \n",
        "corr = diaper_df.corr()\n",
        "#plotting the correlation matrix as a heatmap\n",
        "\n",
        "##WARNING it may take a couple minutes since I'm trying to plot every column\n",
        "\n",
        "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/io/formats/style.py:1089: RuntimeWarning: All-NaN slice encountered\n",
            "  smin = np.nanmin(s.to_numpy()) if vmin is None else vmin\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/io/formats/style.py:1090: RuntimeWarning: All-NaN slice encountered\n",
            "  smax = np.nanmax(s.to_numpy()) if vmax is None else vmax\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMgCcM_FMfJ2",
        "colab_type": "text"
      },
      "source": [
        "### From the correlation heat map above, I can already see what features may be the most correlated to the label column (last col 591) by it's correlation number. I can also see where the all NaN correlation columns are (columns that are constant) and where other features might be correlated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv-wbBG3MfJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating my model variables\n",
        "X = diaper_df.drop([590,591], axis = 1)\n",
        "Y = diaper_df[591]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAwBs_IoMfKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating training/testing variables based on the X and Y dataframes, \n",
        "#test size is 1/4 the size of our input data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, \n",
        "                                                    test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxayawOwMfKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b0d44c70-3feb-407a-f348-5d9fc5c87049"
      },
      "source": [
        "#the class imbalance problem:\n",
        "diaper_df[591].value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1    1319\n",
              " 1      84\n",
              "Name: 591, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2cZxHPLMfKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "22b8e0ed-2328-4398-b3ad-ad2da798b45a"
      },
      "source": [
        "#handling the class imbalance problem with SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_resam, Y_resam = sm.fit_sample(x_train, y_train)\n",
        "print('Resampled diaper_df shape {}'.format(Counter(Y_resam)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resampled diaper_df shape Counter({-1: 986, 1: 986})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCOHIeVgMfKW",
        "colab_type": "text"
      },
      "source": [
        "### Feature Selection and Model Tryout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djnwfe40MfKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WZy42oc1MfKe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e9ac98e-9b71-44f5-da06-547f28ecce86"
      },
      "source": [
        "#using mutual_score_info to determine correlation for each continuous(numerical) variable\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "CORR = []\n",
        "\n",
        "def calc_MI(x, y, bins):\n",
        "    c_xy = np.histogram2d(x, y, bins)[0]\n",
        "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
        "    return mi\n",
        "\n",
        "for i in X.columns:\n",
        "    # Calculation Correlation\n",
        "    X_ = X[i]\n",
        "\n",
        "    #correlation\n",
        "    corr = np.corrcoef(X[i], Y)[0,1]\n",
        "\n",
        "    #mutual information\n",
        "    mi = calc_MI(X[i], Y, 20)\n",
        "    \n",
        "    CORR.append([i, corr, mi])\n",
        "\n",
        "CORR = pd.DataFrame(CORR, columns = ['Variable', 'Correlation', 'Mutual Information'])\n",
        "CORR.sort_values(by=['Mutual Information'], ascending=False)\n",
        "\n",
        "#creating my Filter_Features array, which contains each feature I've selected from the filter method above\n",
        "Filter_Features = []\n",
        "Filter_Features.extend(np.array(CORR.sort_values(by=['Mutual Information'], ascending=False)['Variable'][1:10]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_McFlq1yMfKj",
        "colab_type": "code",
        "colab": {},
        "outputId": "371378fa-6ce5-4c11-cf2f-3ee7301e657f"
      },
      "source": [
        "Filter_Features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[247, 153, 331, 477, 288, 65, 103, 571, 540]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QirySPMfKr",
        "colab_type": "code",
        "colab": {},
        "outputId": "a43c786d-e39e-45d4-a54d-7107879bad8f"
      },
      "source": [
        "Filter_Results = []\n",
        "    \n",
        "#creating my model variables based on the IntDet_model dataframe, and the Class series\n",
        "#test size is 1/4 the size of our input data, and we have 42 variabels total\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_resam[Filter_Features], Y_resam, \n",
        "                                                test_size=0.25)\n",
        "\n",
        "# using the models below to evaluate my selected features\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC(gamma='auto')))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "    \n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(n_splits=10)\n",
        "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    Filter_Results.append([name, cv_results.mean(), cv_results.std()])    \n",
        "\n",
        "Filter_Results = pd.DataFrame(Filter_Results)\n",
        "Filter_Results.columns = ['Model', 'Accuracy Mean', 'Accuracy SD']\n",
        "Filter_Results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy Mean</th>\n",
              "      <th>Accuracy SD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LR</td>\n",
              "      <td>0.612162</td>\n",
              "      <td>0.041122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LDA</td>\n",
              "      <td>0.669595</td>\n",
              "      <td>0.038868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.681757</td>\n",
              "      <td>0.047147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CART</td>\n",
              "      <td>0.847973</td>\n",
              "      <td>0.029179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NB</td>\n",
              "      <td>0.704730</td>\n",
              "      <td>0.043690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.631757</td>\n",
              "      <td>0.040681</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Model  Accuracy Mean  Accuracy SD\n",
              "0    LR       0.612162     0.041122\n",
              "1   LDA       0.669595     0.038868\n",
              "2   KNN       0.681757     0.047147\n",
              "3  CART       0.847973     0.029179\n",
              "4    NB       0.704730     0.043690\n",
              "5   SVM       0.631757     0.040681"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAKDTVzrMfKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Backward Model Selection\n",
        "# Recursive Feature Elimination\n",
        "from sklearn.datasets import make_friedman1\n",
        "from sklearn.feature_selection import RFE \n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "LinReg = LinearRegression()\n",
        "selector = RFE(LinReg, 10, step=1)#select 5 features. Step=1 means each step only remove 1 variable from the model\n",
        "selector = selector.fit(X_resam, Y_resam)\n",
        "# The mask of selected features.\n",
        "# selected features are ranked 1. The 6th is the one that is removed first,\n",
        "# 2nd is the one that is removed last\n",
        "X_resam = pd.DataFrame(X_resam) #need to be a df for below's transformations\n",
        "Wrapper_Features_df = pd.DataFrame(np.stack((X_resam.columns, selector.support_, selector.ranking_)))\n",
        "Wrapper_Features_df = Wrapper_Features_df.transpose()\n",
        "Wrapper_Features_df.columns = ['Variable', 'Support', 'Ranking']\n",
        "Wrapper_Features_df.sort_values(by='Ranking')\n",
        "Wrapper_Features_df = Wrapper_Features_df[Wrapper_Features_df['Ranking']==1]\n",
        "Wrapper_Features_df #feature selection from the wrapper method!\n",
        "\n",
        "Wrapper_Features = list(Wrapper_Features_df['Variable'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ids5bNaFMfK2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "443579df-0da9-4e09-bc5e-f764d7e79441"
      },
      "source": [
        "Wrapper_Features"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 36, 92, 95, 101, 105, 244, 357, 358, 382]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSNr6uw1MfK8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "f37f3ff4-ea2b-48f0-8df9-a9d1d0cc44a8"
      },
      "source": [
        "Wrapper_Results = []\n",
        "    \n",
        "#creating my model variables based on the IntDet_model dataframe, and the Class series\n",
        "#test size is 1/4 the size of our input data, and we have 42 variabels total\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_resam[Wrapper_Features], Y_resam, \n",
        "                                                test_size=0.25)\n",
        "\n",
        "# using the models below to evaluate my selected features\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC(gamma='auto')))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "    \n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(n_splits=10)\n",
        "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    Wrapper_Results.append([name, cv_results.mean(), cv_results.std()])    \n",
        "\n",
        "Wrapper_Results = pd.DataFrame(Wrapper_Results)\n",
        "Wrapper_Results.columns = ['Model', 'Accuracy Mean', 'Accuracy SD']\n",
        "Wrapper_Results"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy Mean</th>\n",
              "      <th>Accuracy SD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LR</td>\n",
              "      <td>0.547642</td>\n",
              "      <td>0.029922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LDA</td>\n",
              "      <td>0.649733</td>\n",
              "      <td>0.035666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.650427</td>\n",
              "      <td>0.032957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CART</td>\n",
              "      <td>0.911418</td>\n",
              "      <td>0.020191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NB</td>\n",
              "      <td>0.586197</td>\n",
              "      <td>0.044065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.531398</td>\n",
              "      <td>0.032234</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Model  Accuracy Mean  Accuracy SD\n",
              "0    LR       0.547642     0.029922\n",
              "1   LDA       0.649733     0.035666\n",
              "2   KNN       0.650427     0.032957\n",
              "3  CART       0.911418     0.020191\n",
              "4    NB       0.586197     0.044065\n",
              "5   SVM       0.531398     0.032234"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdM-i_MMMfLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LASSO\n",
        "from sklearn import linear_model\n",
        "\n",
        "alpha = .01 # Increasing alpha can shrink more variable coefficients to 0\n",
        "clf = linear_model.Lasso(alpha=alpha)\n",
        "clf.fit(X_resam, Y_resam)\n",
        "\n",
        "Embedded_Features_df = pd.DataFrame(np.stack((X_resam.columns, clf.coef_)))\n",
        "Embedded_Features_df = Embedded_Features_df.transpose()\n",
        "Embedded_Features_df.columns = ['Variable', 'Coefficients']\n",
        "Embedded_Features_df.reindex(Embedded_Features_df['Coefficients'].abs().sort_values(ascending=False).index)\n",
        "Embedded_Features_df = Embedded_Features_df[Embedded_Features_df['Coefficients']!=0]\n",
        "Embedded_Features_df #feature selection from the Embedded Method!\n",
        "\n",
        "Embedded_Features = list(Embedded_Features_df['Variable'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9ED5EsfMfLH",
        "colab_type": "code",
        "colab": {},
        "outputId": "8426217d-1b98-480a-ec71-cc823881209d"
      },
      "source": [
        "Embedded_Features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 1.0,\n",
              " 2.0,\n",
              " 3.0,\n",
              " 6.0,\n",
              " 12.0,\n",
              " 14.0,\n",
              " 15.0,\n",
              " 18.0,\n",
              " 21.0,\n",
              " 22.0,\n",
              " 23.0,\n",
              " 24.0,\n",
              " 27.0,\n",
              " 28.0,\n",
              " 32.0,\n",
              " 33.0,\n",
              " 39.0,\n",
              " 40.0,\n",
              " 43.0,\n",
              " 45.0,\n",
              " 46.0,\n",
              " 48.0,\n",
              " 50.0,\n",
              " 51.0,\n",
              " 55.0,\n",
              " 59.0,\n",
              " 60.0,\n",
              " 62.0,\n",
              " 63.0,\n",
              " 64.0,\n",
              " 65.0,\n",
              " 66.0,\n",
              " 68.0,\n",
              " 70.0,\n",
              " 71.0,\n",
              " 72.0,\n",
              " 73.0,\n",
              " 88.0,\n",
              " 90.0,\n",
              " 115.0,\n",
              " 117.0,\n",
              " 133.0,\n",
              " 134.0,\n",
              " 135.0,\n",
              " 136.0,\n",
              " 137.0,\n",
              " 138.0,\n",
              " 139.0,\n",
              " 142.0,\n",
              " 148.0,\n",
              " 151.0,\n",
              " 154.0,\n",
              " 158.0,\n",
              " 159.0,\n",
              " 160.0,\n",
              " 161.0,\n",
              " 162.0,\n",
              " 177.0,\n",
              " 180.0,\n",
              " 182.0,\n",
              " 183.0,\n",
              " 185.0,\n",
              " 187.0,\n",
              " 188.0,\n",
              " 196.0,\n",
              " 197.0,\n",
              " 199.0,\n",
              " 200.0,\n",
              " 201.0,\n",
              " 202.0,\n",
              " 203.0,\n",
              " 205.0,\n",
              " 208.0,\n",
              " 223.0,\n",
              " 225.0,\n",
              " 246.0,\n",
              " 250.0,\n",
              " 268.0,\n",
              " 270.0,\n",
              " 271.0,\n",
              " 272.0,\n",
              " 273.0,\n",
              " 274.0,\n",
              " 286.0,\n",
              " 289.0,\n",
              " 293.0,\n",
              " 294.0,\n",
              " 295.0,\n",
              " 296.0,\n",
              " 297.0,\n",
              " 319.0,\n",
              " 323.0,\n",
              " 324.0,\n",
              " 335.0,\n",
              " 339.0,\n",
              " 341.0,\n",
              " 344.0,\n",
              " 346.0,\n",
              " 361.0,\n",
              " 363.0,\n",
              " 388.0,\n",
              " 406.0,\n",
              " 411.0,\n",
              " 412.0,\n",
              " 413.0,\n",
              " 415.0,\n",
              " 416.0,\n",
              " 417.0,\n",
              " 418.0,\n",
              " 419.0,\n",
              " 421.0,\n",
              " 423.0,\n",
              " 425.0,\n",
              " 427.0,\n",
              " 428.0,\n",
              " 429.0,\n",
              " 430.0,\n",
              " 431.0,\n",
              " 432.0,\n",
              " 433.0,\n",
              " 434.0,\n",
              " 436.0,\n",
              " 437.0,\n",
              " 438.0,\n",
              " 439.0,\n",
              " 442.0,\n",
              " 453.0,\n",
              " 456.0,\n",
              " 460.0,\n",
              " 468.0,\n",
              " 470.0,\n",
              " 471.0,\n",
              " 472.0,\n",
              " 473.0,\n",
              " 474.0,\n",
              " 476.0,\n",
              " 477.0,\n",
              " 482.0,\n",
              " 483.0,\n",
              " 484.0,\n",
              " 485.0,\n",
              " 486.0,\n",
              " 487.0,\n",
              " 488.0,\n",
              " 489.0,\n",
              " 490.0,\n",
              " 491.0,\n",
              " 492.0,\n",
              " 493.0,\n",
              " 494.0,\n",
              " 495.0,\n",
              " 496.0,\n",
              " 497.0,\n",
              " 499.0,\n",
              " 500.0,\n",
              " 510.0,\n",
              " 511.0,\n",
              " 519.0,\n",
              " 521.0,\n",
              " 524.0,\n",
              " 526.0,\n",
              " 539.0,\n",
              " 541.0,\n",
              " 547.0,\n",
              " 548.0,\n",
              " 553.0,\n",
              " 555.0,\n",
              " 557.0,\n",
              " 561.0,\n",
              " 562.0,\n",
              " 564.0,\n",
              " 569.0,\n",
              " 570.0,\n",
              " 572.0,\n",
              " 574.0,\n",
              " 577.0,\n",
              " 581.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RG9Hn1JMfLO",
        "colab_type": "code",
        "colab": {},
        "outputId": "8693b9c9-254e-4c35-d785-33a652995985"
      },
      "source": [
        "Embedded_Results = []\n",
        "    \n",
        "#creating my model variables based on the IntDet_model dataframe, and the Class series\n",
        "#test size is 1/4 the size of our input data, and we have 42 variabels total\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_resam[Embedded_Features], Y_resam, \n",
        "                                                test_size=0.25)\n",
        "\n",
        "# using the models below to evaluate my selected features\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC(gamma='auto')))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "    \n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(n_splits=10)\n",
        "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    Embedded_Results.append([name, cv_results.mean(), cv_results.std()])    \n",
        "\n",
        "Embedded_Results = pd.DataFrame(Embedded_Results)\n",
        "Embedded_Results.columns = ['Model', 'Accuracy Mean', 'Accuracy SD']\n",
        "Embedded_Results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\natal\\AnacondaNEW\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy Mean</th>\n",
              "      <th>Accuracy SD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LR</td>\n",
              "      <td>0.932432</td>\n",
              "      <td>0.008547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LDA</td>\n",
              "      <td>0.906081</td>\n",
              "      <td>0.018492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.770946</td>\n",
              "      <td>0.025720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CART</td>\n",
              "      <td>0.910811</td>\n",
              "      <td>0.021324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NB</td>\n",
              "      <td>0.885811</td>\n",
              "      <td>0.022092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.535811</td>\n",
              "      <td>0.006081</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Model  Accuracy Mean  Accuracy SD\n",
              "0    LR       0.932432     0.008547\n",
              "1   LDA       0.906081     0.018492\n",
              "2   KNN       0.770946     0.025720\n",
              "3  CART       0.910811     0.021324\n",
              "4    NB       0.885811     0.022092\n",
              "5   SVM       0.535811     0.006081"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiVkzG2XMfLV",
        "colab_type": "code",
        "colab": {},
        "outputId": "a82e979f-b4a2-4b12-837b-d1cd13270425"
      },
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.barh(Filter_Results['Model'], Filter_Results['Accuracy Mean'], alpha=.3, label='Filter')\n",
        "plt.barh(Wrapper_Results['Model'], Wrapper_Results['Accuracy Mean'], alpha=.3, label='Wrapper')\n",
        "plt.barh(Embedded_Results['Model'], Embedded_Results['Accuracy Mean'], alpha=.3, label='Embedded')\n",
        "plt.legend()\n",
        "plt.xlabel('Accuracy Mean')\n",
        "plt.ylabel('Model')\n",
        "plt.title('Showing Accuracy per Method')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4IAAAFNCAYAAABVKNEpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7xVZb3v8c9PUFEkTMHygoJtLxkSECAgKd7Na1ZmlKbZLj372EXb59T2pJHbk7teapbZMXQbunMreb8cNbV9MOWigq5EwUzDC4YmF1FQVOB3/phj0WS5WCyUOedaa3zer9d6Ocd4njHGb841Ir48z3hmZCaSJEmSpPLYqNEFSJIkSZLqyyAoSZIkSSVjEJQkSZKkkjEISpIkSVLJGAQlSZIkqWQMgpIkSZJUMgZBSdIaIuKkiHigxte4NCLOquU11DFFxNiImLeBztU/IjIium+I80lSmRgEJamEImJMREyNiCURsSgipkTE8HpdPzNPzcx/rdX5I6JnRCyNiDtqdY2uLiImFiHrqBb7Lyr2n9TO82RE/ENNipQkvWcGQUkqmYj4AHA7cDGwFbA98EPgrUbWtYF9jsr7OTgitq3nhTvj6FQbNT8FnNii37HAM/WoS5JUOwZBSSqfXQEy85rMXJmZb2bm3Zn5WHWniDg/IhZHxNyI+FTV/u0i4tZiJPHpiPhasb9HRLwZEX2K7e9HxIoieBIR50bERcXriRFxbvF6bETMi4jvRMTfImJ+RHyl6npbR8RtEfFaRDxcnGddU1dPBC4FHgO+1OJ9NY+GvhoRLzSPbEXEZhFxQUQ8V4yUPlDse9dUxoh4NiIOLF6Pj4jrI+I3EfEacFJEjIiIacU15kfELyJik6rjPxYR9xSf4csRcWZEfDgi3oiIrav6fSIiXomIjVu+warrToqI1yPikYj4eIvf0w3F8XMj4putHLu65rV8jrcBe0fEB4vtQ4vP9KUWtZwcEXOK++V3EbFTsf8PRZc/FiO0x1Uds7bfd++IuKqo+7niPtqoaOtW3JcLIuIvwOFrqVuStA4GQUkqn6eAlRFxZUR8quov+dX2Av4E9AF+Avx7RETRdg0wD9iOysjbjyLigMxcDjwM7Fv02wd4Dti7avu+tdT0YaA3ldHJrwKXVNV1CbCs6HMiVSNUrYmIHYGxwNXFz5dbtN1JZTS0LzAYaCqazwc+AYymMlL6P4FVbV2rytHA9cCWxTVXAqdT+fxGAQcA/1TU0Au4F7iLymf4D8DvM/MlYDLw+arzHg9cm5nvtHHd64p6/xO4OSI2LoLTbcAfqXymBwDfjohD2qi5NcuBW4EvFNtfBq6q7hARnwbOBD5D5TO9n8o9QmbuU3T7eGZukZmTiu22ft8XF207U7mXvgw0B8WvAUcAQ4BhVO4/SdJ7YBCUpJLJzNeAMUAClwGvFCN8H6rq9lxmXpaZK4ErgW2BD0VEv+LY72bm8sxsAi4HTiiOuw/Yt5hCOAj4ebHdAxhOJSS05h3gnMx8JzPvAJYCu0VEN+CzwA8y843MnF3U05YvA48Vfa8BPhYRQ4q2LwH3FqOh72TmwsxsKoLTycC3MvPFYqR0ama2d7rstMy8OTNXFSOsMzNzemauyMxngV/x94B8BPBSZl5QfIavZ+aDRduVVMIfxXsfB/xHG9edmZnXF0HxQqAHMJLKZ903M8/JzLcz8y9UftdfqDp2jZrbuMZVwJcjonfxHm5u0X4KcF5mzsnMFcCPgMHNo4Jr0dbv+zjgX4rP5VngAv5+f30euCgzX8jMRcB5bVxDktQGg6AklVDxl/aTMnMHYCCVkamLqrq8VNX3jeLlFkW/RZn5elXf56iM7EAlCI4FhgKzgHuohIeRwNOZuWAtJS0sQkSzN4rr9QW6Ay9UtVW/bs2XKUa4MvOvRU3No4j9aP35tj5UQtR7ffZtjZoiYteIuD0iXiqmXv6ouEZbNQDcAuwRETsDBwFLMvOh9lw3M1fx95HanYDtiqmpr0bEq1RG7T7U2rFtycwHqPwevg/c3kpo3An4WdV1FgHB3++J1qzt990H2ITKPdWs+v7arkXd1f0kSevBIChJJZeZTwITqQTCdfkrsFUxvbHZjsCLxeupwG7AMcB9xajcjlSe5VrbtNC2vAKsAHao2tdvbZ0jYjSwC/AvRQh7ico013HFKOULwEdaOXQBlWmQrbUtAzavukY3KsGoWrbY/j/Ak8AumfkBKiGseWrt2mqgmF77WyojlyfQ9mggVH0WxajmDlR+Ry8AczNzy6qfXpl5WBs1t+U3wHdoMS208AJwSotrbZaZU9fj/M0WUBktrB5NrL6/5rPm73/H93ANSRIGQUkqnYjYvVioY4diux+VKYjT13VsZr5AJeydF5XFYQZRecareQTuDWAm8N/5e/CbSmX64HoHwWJq6o3A+IjYPCJ2p+qZv1acSGUUcg8qz/8NphJwNwc+VdR5YER8PiK6R2UhmsHFaNoVwIXFIivdImJURGxK5ZnKHhFxeLFoy/eBTddRei/gNWBpUfN/q2q7HfhwRHw7IjaNiF4RsVdV+1VUFm85ikoAa8snIuIzRcj9NpWVUqcDDwGvRcR3o7LgTbeIGBjv/StCfk5lhPIPrbRdSiV4fwxWL/ZybFX7y1Se91un4vf9W+B/F5/LTsAZ/P1z+C3wzYjYoXim8Hvv6d1IkgyCklRCr1MZJXswIpZRCQ6PUxnxaY9xQH8qI083UXl+756q9vuAjamEkebtXrQeItrjNCqLh7xEZYTsGlr5qoviOcTPAxdn5ktVP3OL407MzOeBw6i810VUFoppXmnzn6lMZ324aPsxsFFmLqGy0MvlVEamllGZgtmWfwa+SOWzvgxoXiSFYlrtQcCRxXv6M7BfVfsUKovUPFI8I9eWW6g8U7eYygjiZ4rn7lYW5x8MzKUy0nY5lc9xvWXmosz8fWa+axQxM2+i8lldW0yDfZxK6G42HriymDr6+ZbHt+IbVD7jvwAPUFkE54qi7TLgd1QWwXmEyj8SSJLeg2jlz3RJkjqsiPgx8OHMbHP10M4sIv4L+M/MvLyNPuOBf8jM4+tWmCSpy3BEUJLUoRVTWQdFxQgqU1FvanRdtVJM3xxK1SiiJEkbWvdGFyBJ0jr0ojIddDvgb1S+TuCWhlZUIxFxJfBpKl9j8fq6+kuS9F45NVSSJEmSSsapoZIkSZJUMgZBSZIkSSqZLvuMYJ8+fbJ///6NLkOSJEmSGmLmzJkLMrNva21dNgj279+fGTNmNLoMSZIkSWqIiHhubW1ODZUkSZKkkjEISpIkSVLJGAQlSZIkqWS67DOCkiRJkjqGd955h3nz5rF8+fJGl9Il9ejRgx122IGNN9643ccYBCVJkiTV1Lx58+jVqxf9+/cnIhpdTpeSmSxcuJB58+YxYMCAdh/n1FBJkiRJNbV8+XK23nprQ2ANRARbb731eo+2GgQlSZIk1ZwhsHbey2drEJQkSZLU5XXr1o3Bgwev/nn22WcZPXo0AM8++ywDBw4EoKmpiTvuuKORpdaFzwhKkiRJqqt7Z7+8Qc934B4fWmefzTbbjKampjX2TZ069V39mpqamDFjBocddli7r79ixQq6d+9c0coRQUmSJEmltMUWW6yx/fbbb3P22WczadIkBg8ezKRJk1i2bBknn3wyw4cPZ8iQIdxyyy0ATJw4kWOPPZYjjzySgw8+uBHlvy+dK7auhwVL5nPFbec0ugxJkjqkoZvt0ugSam7BdvvX/BrtGYWQ1DG8+eabDB48GIABAwZw0003vavPJptswjnnnMOMGTP4xS9+AcCZZ57J/vvvzxVXXMGrr77KiBEjOPDAAwGYNm0ajz32GFtttVX93sgG0mWDoCRJkiQ1a21qaHvcfffd3HrrrZx//vlAZQXU559/HoCDDjqoU4ZAMAhKkiRJ0lplJjfccAO77bbbGvsffPBBevbs2aCq3j+fEZQkSZKkQq9evXj99ddXbx9yyCFcfPHFZCYAjz76aKNK26AMgpIkSZJU2G+//Zg9e/bqxWLOOuss3nnnHQYNGsTAgQM566yzGl3iBhHNybar6b/L9nn2hac0ugxJkjokF4vZMFwsRmqfOXPm8NGPfrTRZXRprX3GETEzM4e11t8RQUmSJEkqGYOgJEmSJJWMQVCSJEmSSsYgKEmSJEklYxCUJEmSpJIxCEqSJElSyRgEJUmSJHVpp59+OhdddNHq7UMOOYR//Md/XL39ne98hwsvvLARpTVM90YXIEmSJKlk/nTnhj3fbp9qs3n06NFcd911fPvb32bVqlUsWLCA1157bXX71KlT1wiKK1asoHv3+kallStX0q1bt7pdr64jghHxvyLiiYh4LCKaIuLOiDivRZ/BETGneP1sRNzfor0pIh6vZ92SJEmSOq+9996bqVOnAvDEE08wcOBAevXqxeLFi3nrrbeYM2cOp59+OmeeeSb77rsvP/vZz7jtttvYa6+9GDJkCAceeCAvv/wyAOPHj+eEE05g//33Z5ddduGyyy4DYPLkyeyzzz4cc8wx7LHHHpx66qmsWrUKgLvvvptRo0YxdOhQjj32WJYuXQpA//79OeeccxgzZgzXXXddXT+TusXciBgFHAEMzcy3IqIP8DHg18C/VHX9AvCfVdu9IqJfZr4QER+tV72SJEmSuobtttuO7t278/zzzzN16lRGjRrFiy++yLRp0+jduzeDBg1io4024tVXX+W+++4DYPHixUyfPp2I4PLLL+cnP/kJF1xwAQCPPfYY06dPZ9myZQwZMoTDDz8cgIceeojZs2ez0047ceihh3LjjTcyduxYzj33XO6991569uzJj3/8Yy688ELOPvtsAHr06MEDDzxQ98+knuOd2wILMvMtgMxcANwXEa9GxF6Z+WDR7/PAIVXH/RY4DjgfGAdcA5xQv7IlSZIkdXbNo4JTp07ljDPO4MUXX2Tq1Kn07t2b0aNHM336dI477rjV/efNm8dxxx3H/PnzefvttxkwYMDqtqOPPprNNtuMzTbbjP3224+HHnqILbfckhEjRrDzzjsDMG7cOB544AF69OjB7Nmz2XvvvQF4++23GTVq1OpzVV+znuo5NfRuoF9EPBURv4yIfYv911AZBSQiRgILM/PPVcddD3ymeH0kcNvaLhARX4+IGRExY+mSZRv+HUiSJEnqlEaPHs3UqVOZNWsWAwcOZOTIkUybNo2pU6euDmk9e/Zc3f8b3/gGp512GrNmzeJXv/oVy5cvX90WEWucu3m7tf2ZyUEHHURTUxNNTU3Mnj2bf//3f1/dp/qa9VS3IJiZS4FPAF8HXgEmRcRJwLXA5yJiIyqB8JoWhy4CFkfEF4A5wBttXGNCZg7LzGFb9G7MBypJkiSp49l77725/fbb2WqrrejWrRtbbbUVr776KtOmTVtjhK7ZkiVL2H777QG48sor12i75ZZbWL58OQsXLmTy5MkMHz4cqEwNnTt3LqtWrWLSpEmMGTOGkSNHMmXKFJ5++mkA3njjDZ566qkav9t1q+tiMZm5MjMnZ+YPgNOAz2bmC8CzwL7AZ6lMBW1pEnAJ7w6JkiRJkrROe+65JwsWLGDkyJFr7Ovduzd9+vR5V//x48dz7LHH8slPfvJd7SNGjODwww9n5MiRnHXWWWy33XYAjBo1iu9973sMHDiQAQMGcMwxx9C3b18mTpzIuHHjGDRoECNHjuTJJ5+s7Ztth3ouFrMbsKpq2udg4Lni9TXAT4FnMnNeK4ffROUZw98B29W6VkmSJEk1tI6ve6iFbt26rfGVEQATJ05c/Xry5MlrtB199NEcffTRrZ5r1113ZcKECe/av/nmmzNp0qR37d9///15+OGH37X/2WefXXfhNVLPxWK2AC6OiC2BFcDTVKaJAlwH/Az4RmsHZubrwI/h3fNuJUmSJEnrp25BMDNnAqPX0vYKsHEr+/u3su9ZYOAGLk+SJEmS1mn8+PGt7h87dixjx46tay3vR12fEZQkSZIkNZ5BUJIkSZJKxiAoSZIkSSVjEJQkSZKkkjEISpIkSeryunXrxuDBg1f//Nu//Vu7j508eTJHHHHEe752W8f379+fBQsWtPtcEydO5LTTTnvPtTSr59dHSJIkSRKTX5i8Qc83tt/YdfbZbLPNaGpq2qDX7cwcEZQkSZJUWv379+fMM89k1KhRDBs2jEceeYRDDjmEj3zkI1x66aWr+7322mscc8wx7LHHHpx66qmsWrUKgLvvvptRo0YxdOhQjj32WJYuXQrAXXfdxe67786YMWO48cYbV59n4cKFHHzwwQwZMoRTTjmFzFzd9pvf/IYRI0YwePBgTjnlFFauXAnAr3/9a3bddVf23XdfpkyZskHet0FQkiRJUpf35ptvrjE1dNKkSavb+vXrx7Rp0/jkJz/JSSedxPXXX8/06dM5++yzV/d56KGHuOCCC5g1axbPPPMMN954IwsWLODcc8/l3nvv5ZFHHmHYsGFceOGFLF++nK997Wvcdttt3H///bz00kurz/PDH/6QMWPG8Oijj3LUUUfx/PPPAzBnzhwmTZrElClTaGpqolu3blx99dXMnz+fH/zgB0yZMoV77rmH2bNnb5DPw6mhkiRJkrq8tqaGHnXUUQDsueeeLF26lF69etGrVy969OjBq6++CsCIESPYeeedARg3bhwPPPAAPXr0YPbs2ey9994AvP3224waNYonn3ySAQMGsMsuuwBw/PHHM2HCBAD+8Ic/rB4hPPzww/ngBz8IwO9//3tmzpzJ8OHDgUpw3WabbXjwwQcZO3Ysffv2BeC4447jqaeeet+fh0FQkiRJUqltuummAGy00UarXzdvr1ixAoCIWOOYiCAzOeigg7jmmmvWaGtqanpX/5bHtpSZnHjiiZx33nlr7L/55pvbPNd75dRQSZIkSVqHhx56iLlz57Jq1SomTZrEmDFjGDlyJFOmTOHpp58G4I033uCpp55i9913Z+7cuTzzzDMAawTFffbZh6uvvhqAO++8k8WLFwNwwAEHcP311/O3v/0NgEWLFvHcc8+x1157MXnyZBYuXMg777zDddddt0HeT5cdEezTe1tOPvLsdXeUJEmS1OU1PyPY7NBDD12vr5AYNWoU3/ve95g1axb77LMPxxxzDBtttBETJ05k3LhxvPXWWwCce+657LrrrkyYMIHDDz+cPn36MGbMGB5//HEAfvCDHzBu3DiGDh3Kvvvuy4477gjAHnvswbnnnsvBBx/MqlWr2HjjjbnkkksYOXIk48ePZ9SoUWy77bYMHTp09SIy70dUr1LTlQwbNixnzJjR6DIkSZKk0pszZw4f/ehHG11Gl9baZxwRMzNzWGv9nRoqSZIkSSVjEJQkSZKkkjEISpIkSVLJGAQlSZIk1VxXXZukI3gvn61BUJIkSVJN9ejRg4ULFxoGayAzWbhwIT169Fiv47rs10dIkiRJ6hh22GEH5s2bxyuvvNLoUrqkHj16sMMOO6zXMQZBSZIkSTW18cYbM2DAgEaXoSpdNgguWDKfK247p9FlSJKk9fDa1oMaXYLaaXC/LRtdgkpgbL+xjS6hy/IZQUmSJEkqGYOgJEmSJJWMQVCSJEmSSsYgKEmSJEklYxCUJEmSpJIxCEqSJElSyRgEJUmSJKlkDIKSJEmSVDIGQUmSJEkqGYOgJEmSJJWMQVCSJEmSSsYgKEmSJEkl02GCYERkRFxQtf3PETG+eD0+Il6MiKaIeDIi/k9EdJjaJUmSJKkz6Uhh6i3gMxHRZy3tP83MwcAewJ7AvnWrTJIkSZK6kI4UBFcAE4DT19FvE6AHsLjmFUmSJElSF9SRgiDAJcCXIqJ3K22nR0QTMB94KjOb6luaJEmSJHUNHSoIZuZrwFXAN1tpbp4aug3QMyK+0LJDRHw9ImZExIylS5bVuFpJkiRJ6pw6VBAsXAR8FejZWmNmvgPcBezTStuEzByWmcO26N3q4ZIkSZJUeh0uCGbmIuC3VMLgu0REAKOBZ+pZlyRJkiR1FR0uCBYuAFquHtr8jODjQHfgl3WvSpIkSZK6gO6NLqBZZm5R9fplYPOq7fHA+PpXJUmSJEldT0cdEZQkSZIk1YhBUJIkSZJKxiAoSZIkSSVjEJQkSZKkkjEISpIkSVLJGAQlSZIkqWQMgpIkSZJUMgZBSZIkSSoZg6AkSZIklYxBUJIkSZJKxiAoSZIkSSVjEJQkSZKkkune6AJqpU/vbTn5yLMbXYYkSZIkdTiOCEqSJElSyRgEJUmSJKlkDIKSJEmSVDIGQUmSJEkqGYOgJEmSJJWMQVCSJEmSSsYgKEmSJEklYxCUJEmSpJLpsl8ov2DJfK647ZxGlyFJkiR1Wjv37dnoEmpi7Af3aL1ht0/Vt5AGckRQkiRJkkrGIChJkiRJJWMQlCRJkqSSMQhKkiRJUskYBCVJkiSpZAyCkiRJklQyBkFJkiRJKhmDoCRJkiSVjEFQkiRJkkrGIChJkiRJJWMQlCRJkqSSMQhKkiRJUsnUNAhGxIcj4tqIeCYiZkfEHRGxa9F2ekQsj4jeVf3HRsSSiHg0Ip6MiPMjYs+IaCp+FkXE3OL1vbWsXZIkSZK6qpoFwYgI4CZgcmZ+JDP3AM4EPlR0GQc8DBzT4tD7M3MIMAQ4AvhAZg7OzMHArcD/KLYPrFXtkiRJktSV1XJEcD/gncy8tHlHZjZl5v0R8RFgC+D7VALhu2Tmm0ATsH0Na5QkSZKk0qllEBwIzFxL2zjgGuB+YLeI2KZlh4j4ILAL8IeaVShJkiRJJdSoxWK+AFybmauAG4Fjq9o+GRGPAS8Bt2fmS+09aUR8PSJmRMSMpUuWbdiKJUmSJKmLqGUQfAL4RMudETGIykjfPRHxLJVQWD099P7MHATsCfy3iBjc3gtm5oTMHJaZw7bo3fN9FS9JkiRJXVUtg+B/AZtGxNead0TEcOBnwPjM7F/8bAdsHxE7VR+cmU8B5wHfrWGNkiRJklQ6NQuCmZlUVgQ9qPj6iCeA8cBYKquJVruJyshgS5cC+0TEgFrVKUmSJEll072WJ8/MvwKfb0e/M6o2J1ftf5OqVUMz86QNWJ4kSZIklVKjFouRJEmSJDWIQVCSJEmSSsYgKEmSJEkl0+YzghGxVVvtmblow5YjSZIkSaq1dS0WMxNIIFppS2DnDV6RJEmSJKmm2gyCmenXNkiSJElSF9OuZwSj4viIOKvY3jEiRtS2NEmSJElSLbR3sZhfAqOALxbbrwOX1KQiSZIkSVJNtfcL5ffKzKER8ShAZi6OiE1qWJckSZIkqUbaOyL4TkR0o7JADBHRF1hVs6okSZIkSTXT3iD4c+AmYJuI+N/AA8CPalaVJEmSJKlm2jU1NDOvjoiZwAFUvkri05k5p6aVSZIkSZJqIjJz7Y2d+Avlhw0bljNmzGh0GZIkSZLUEBExMzOHtda2Pl8ovyOwuHi9JfA84PcMSpIkSVIn0+Yzgpk5IDN3Bn4HHJmZfTJza+AI4MZ6FChJkiRJ2rDau1jM8My8o3kjM+8E9q1NSZIkSZKkWmrv9wguiIjvA7+hMlX0eGBhzaqSJEmSJNVMe0cExwF9qXyFxM3ANsU+SZIkSVIn096vj1gEfCsiPgCsysyltS1LkiRJklQr7RoRjIg9I+JRYBbwRETMjIiBtS1NkiRJklQL7X1G8FfAGZn5/wAiYiwwARhdo7retwVL5nPFbec0ugxJkqQOYee+Pd//Sbb/xPs/Rycxtt/YRpcg1VR7nxHs2RwCATJzMrAB/jSRJEmSJNVbe0cE/xIRZwH/UWwfD8ytTUmSJEmSpFpq74jgyVRWDb2RysqhfYGv1KooSZIkSVLttHfV0MXAN2tciyRJkiSpDtoMghFxa1vtmXnUhi1HkiRJklRr6xoRHAW8AFwDPAhEzSuSJEmSJNXUuoLgh4GDgHHAF4H/C1yTmU/UujBJkiRJUm20uVhMZq7MzLsy80RgJPA0MDkivlGX6iRJkiRJG9w6F4uJiE2Bw6mMCvYHfk5l9VBJkiRJUie0rsVirgQGAncCP8zMx+tSlSRJkiSpZtY1IngCsAzYFfhmxOq1YgLIzPxADWuTJEmSJNVAm0EwM9v7hfOSJEmSpE6irkEvIpZWvT4sIv4cETtGxPiIeCMitllL34yIC6q2/zkixtetcEmSJEnqQhoy4hcRBwAXA4dm5vPF7gXAd9ZyyFvAZyKiTz3qkyRJkqSurO5BMCI+CVwGHJ6Zz1Q1XQEcFxFbtXLYCmACcHodSpQkSZKkLq3eQXBT4Bbg05n5ZIu2pVTC4LfWcuwlwJcioncN65MkSZKkLq/eQfAdYCrw1bW0/xw4MSLetRppZr4GXAV8c20nj4ivR8SMiJixdMmyDVGvJEmSJHU59Q6Cq4DPA8Mj4syWjZn5KvCfwD+t5fiLqITInq01ZuaEzByWmcO26N1qF0mSJEkqvbo/I5iZbwBHUJnm2drI4IXAKbTy1RaZuQj4LWsfUZQkSZIkrUNDVg0tAt2hwPcj4ugWbQuAm6g8T9iaCwBXD5UkSZKk96jNL5Tf0DJzi6rXLwADis1bWvQ7AzhjLce9DGxe20olSZIkqetqyIigJEmSJKlxDIKSJEmSVDIGQUmSJEkqGYOgJEmSJJWMQVCSJEmSSsYgKEmSJEklYxCUJEmSpJIxCEqSJElSyRgEJUmSJKlkDIKSJEmSVDIGQUmSJEkqGYOgJEmSJJVM90YXUCt9em/LyUee3egyJEmSJKnDcURQkiRJkkrGIChJkiRJJWMQlCRJkqSSMQhKkiRJUskYBCVJkiSpZAyCkiRJklQyBkFJkiRJKhmDoCRJkiSVTJf9QvkFS+ZzxW3nNLoMSZKkDeq1rQetsT2435YNqkTqPMb2G9voEjocRwQlSZIkqWQMgpIkSZJUMgZBSZIkSSoZg6AkSZIklYxBUJIkSZJKxiAoSZIkSSVjEJQkSZKkkjEISpIkSVLJGAQlSZIkqWQMgpIkSZJUMgZBSZIkSSoZg6AkSZIklUzdg2BELG1l3/iIeDEimiLizxFxY0Ts0aLPkIjIiDikftVKkiRJUtfTkUYEf5qZgzNzF2AS8F8R0beqfRzwQPFfSZIkSSKefo8AAAuESURBVNJ71JGC4GqZOQm4G/giQEQE8DngJODgiOjRuOokSZIkqXPrkEGw8Aiwe/F6b2BuZj4DTAYOa1RRkiRJktTZdeQgGFWvxwHXFq+vZS3TQyPi6xExIyJmLF2yrNb1SZIkSVKn1L3RBbRhCDAjIroBnwWOioj/RSUgbh0RvTLz9eoDMnMCMAGg/y7bZ70LliRJkqTOoEOOCEbEZ4GDgWuAA4E/Zma/zOyfmTsBNwCfbmSNkiRJktRZNSIIbh4R86p+zij2n9789RHA8cD+mfkKlWmgN7U4xw0UC8lIkiRJktZP3aeGZubawuf4tfQ/qZV9twK3briqJEmSJKk8OuTUUEmSJElS7RgEJUmSJKlkDIKSJEmSVDIGQUmSJEkqGYOgJEmSJJWMQVCSJEmSSsYgKEmSJEklYxCUJEmSpJIxCEqSJElSyRgEJUmSJKlkDIKSJEmSVDIGQUmSJEkqme6NLqBW+vTelpOPPLvRZUiSJElSh+OIoCRJkiSVjEFQkiRJkkrGIChJkiRJJWMQlCRJkqSSMQhKkiRJUskYBCVJkiSpZAyCkiRJklQyXfZ7BBcsmc8Vt53T6DIkSSq917Ye1OgSWjW435aNLkFSJze239hGl/CeOSIoSZIkSSVjEJQkSZKkkjEISpIkSVLJGAQlSZIkqWQMgpIkSZJUMgZBSZIkSSoZg6AkSZIklYxBUJIkSZJKxiAoSZIkSSVjEJQkSZKkkjEISpIkSVLJGAQlSZIkqWQMgpIkSZJUMh0uCEbE0lb2jY+IFyOiKSJmR8S4RtQmSZIkSV1BhwuCbfhpZg4GjgZ+FREbN7ogSZIkSeqMOlMQBCAz/wy8AXyw0bVIkiRJUmfU6YJgRAwF/pyZf2ul7esRMSMiZixdsqwB1UmSJElSx9eZguDpEfEn4EFgfGsdMnNCZg7LzGFb9O5Z1+IkSZIkqbPoTEHwp5m5G3AccFVE9Gh0QZIkSZLUGXWmIAhAZt4IzABObHQtkiRJktQZdcQguHlEzKv6OaOVPucAZ0RER6xfkiRJkjq07o0uoKXMXGe4y8yZwG51KEeSJEmSuhxH1CRJkiSpZAyCkiRJklQyBkFJkiRJKhmDoCRJkiSVjEFQkiRJkkrGIChJkiRJJWMQlCRJkqSSMQhKkiRJUskYBCVJkiSpZAyCkiRJklQyBkFJkiRJKpnujS6gVvr03paTjzy70WVIkiRJUofjiKAkSZIklYxBUJIkSZJKxiAoSZIkSSVjEJQkSZKkkjEISpIkSVLJGAQlSZIkqWQMgpIkSZJUMgZBSZIkSSoZg6AkSZIklYxBUJIkSZJKxiAoSZIkSSVjEJQkSZKkkonMbHQNNRERrwN/anQdUjv0ARY0ugipHbxX1Vl4r6qz8F5Vre2UmX1ba+he70rq6E+ZOazRRUjrEhEzvFfVGXivqrPwXlVn4b2qRnJqqCRJkiSVjEFQkiRJkkqmKwfBCY0uQGon71V1Ft6r6iy8V9VZeK+qYbrsYjGSJEmSpNZ15RFBSZIkSVIrOn0QjIhDI+JPEfF0RHyvlfaIiJ8X7Y9FxNBG1Cm14179UnGPPhYRUyPi442oU1rXvVrVb3hErIyIz9WzPqlZe+7ViBgbEU0R8URE3FfvGiVo198BekfEbRHxx+Je/Uoj6lS5dOqpoRHRDXgKOAiYBzwMjMvM2VV9DgO+ARwG7AX8LDP3akC5KrF23qujgTmZuTgiPgWM915VvbXnXq3qdw+wHLgiM6+vd60qt3b+ubolMBU4NDOfj4htMvNvDSlYpdXOe/VMoHdmfjci+lL5LuwPZ+bbjahZ5dDZRwRHAE9n5l+K/6FcCxzdos/RwFVZMR3YMiK2rXehKr113quZOTUzFxeb04Ed6lyjBO37cxUq/8B2A+BfqtUo7blXvwjcmJnPAxgC1SDtuVcT6BURAWwBLAJW1LdMlU1nD4LbAy9Ubc8r9q1vH6nW1vc+/CpwZ00rklq3zns1IrYHjgEurWNdUkvt+XN1V+CDETE5ImZGxJfrVp30d+25V38BfBT4KzAL+FZmrqpPeSqr7o0u4H2KVva1nOvanj5SrbX7PoyI/agEwTE1rUhqXXvu1YuA72bmyso/XksN0Z57tTvwCeAAYDNgWkRMz8ynal2cVKU99+ohQBOwP/AR4J6IuD8zX6t1cSqvzh4E5wH9qrZ3oPIvKevbR6q1dt2HETEIuBz4VGYurFNtUrX23KvDgGuLENgHOCwiVmTmzfUpUQLa/3eABZm5DFgWEX8APk7leS2pXtpzr34F+LesLN7xdETMBXYHHqpPiSqjzj419GFgl4gYEBGbAF8Abm3R51bgy8XqoSOBJZk5v96FqvTWea9GxI7AjcAJ/mu1Gmid92pmDsjM/pnZH7ge+CdDoBqgPX8HuAX4ZER0j4jNqSwaN6fOdUrtuVefpzJyTUR8CNgN+Etdq1TpdOoRwcxcERGnAb8DulFZue6JiDi1aL8UuIPKiqFPA29Q+RcXqa7aea+eDWwN/LIYaVmRmcMaVbPKqZ33qtRw7blXM3NORNwFPAasAi7PzMcbV7XKqJ1/rv4rMDEiZlGZSvrdzFzQsKJVCp366yMkSZIkSeuvs08NlSRJkiStJ4OgJEmSJJWMQVCSJEmSSsYgKEmSJEklYxCUJEmSpJIxCEqSuoyIOCYiMiJ2b3Qt6ysi+he1/2vVvj4R8U5E/KKRtUmSuh6DoCSpKxkHPEDlC5trJiK61ejUfwGOqNo+FniiRteSJJWYQVCS1CVExBbA3sBXqQqCEdEtIs6PiFkR8VhEfKPYPzwipkbEHyPioYjoFREnVY++RcTtETG2eL00Is6JiAeBURFxdkQ8HBGPR8SEiIii3z9ExL3FeR+JiI9ExH9ExNFV5706Io5q5W28CcyJiGHF9nHAb6uO6xsRNxTXfTgi9i72jyjey6PFf3cr9p8UETdGxF0R8eeI+MkG+KglSV2AQVCS1FV8GrgrM58CFkXE0GL/14EBwJDMHARcHRGbAJOAb2Xmx4EDqYSwtvQEHs/MvTLzAeAXmTk8MwcCm/H3kbyrgUuK844G5gOXA18BiIjexf471nKda4EvRMQOwErgr1VtPwN+mpnDgc8W5wV4EtgnM4cAZwM/qjpmMJVAuSdwXET0W8f7lCSVQPdGFyBJ0gYyDrioeH1tsf0IlZB3aWauAMjMRRGxJzA/Mx8u9r0GUAzqrc1K4Iaq7f0i4n8CmwNbAU9ExGRg+8y8qTjv8qLvfRFxSURsA3wGuKG5nlbcBfwr8DKVsFrtQGCPqjo/EBG9gN7AlRGxC5DAxlXH/D4zlxTvbzawE/BCW29UktT1GQQlSZ1eRGwN7A8MjIgEugFZBLWgEo7WOKSVfQArWHO2TI+q18szc2VxvR7AL4FhmflCRIwv+raVJP8D+BKVaasnr61TZr4dETOB7wAfA46sat4IGJWZa4xeRsTFwP/LzGMioj8wuar5rarXK/H/+yVJODVUktQ1fA64KjN3ysz+mdkPmAuMAe4GTo2I7gARsRWVqZTbRcTwYl+vov1ZYHBEbFRMoRyxlus1B8QFxbOJn4PVI4vzIuLTxXk3jYjNi74TgW8X/da1AMwFwHczc2GL/XcDpzVvRMTg4mVv4MXi9UnrOLckSQZBSVKXMA64qcW+G4AvUnmO7nngsYj4I/DFzHybynNzFxf77qES7qZQCZCzgPOpTC19l8x8Fbis6Hcz8HBV8wnANyPiMWAq8OHimJeBOcCv1/VmMvOJzLyylaZvAsOKRW9mA6cW+38CnBcRU6iMhkqS1KbIbG1mjCRJ2pCKkcFZwNDmZ/YkSWoURwQlSaqxiDiQynTUiw2BkqSOwBFBSZIkSSoZRwQlSZIkqWQMgpIkSZJUMgZBSZIkSSoZg6AkSZIklYxBUJIkSZJKxiAoSZIkSSXz/wGm8khh2qArewAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-A7K53aMfLZ",
        "colab_type": "text"
      },
      "source": [
        "In the bar chart above, it looks at first like Embedded method features have the highest accuracy in this senario. However, it's a bit of a red flag that most of the model methods have the highest accuracy with the Embedded features. The Embedded features may be overfitting the dataset. \n",
        "\n",
        "Instead, I will use the Wrapper method features because they scored the 2nd highest in accuracy mean accross all feature selection methods and model types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSj3SRO4MfLa",
        "colab_type": "text"
      },
      "source": [
        "# Begin Milestone02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY6aupDjMfLb",
        "colab_type": "text"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "I had previously determined for Milestone01 that the features selected by the wrapper method to predict the target variable (diaper_df[591]) yeilded the best model results. Therefore, for Milestone02 I will be using only those features selected by the wrapper method (Wrapper_Features) in the decision tree, ensemble, and SVM models to come.\n",
        "\n",
        "After testing each of the models: Decision Tree, Ensemble, and SVM, I noticed that the accuracy of each model was decreasing as I ran the models. I'm wondering if I did anything to make this happen and how to fix this? I'm confused on how to know when a result is real or fake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGZW8tCjMfLb",
        "colab_type": "text"
      },
      "source": [
        "#### Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnmHjgcxMfLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the features, X, and the target is still Y from before\n",
        "X = X_resam[Wrapper_Features] \n",
        "Y = Y_resam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Tcl9uRMfLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ensure that the decision tree is deterministic\n",
        "import numpy as np\n",
        "np.random.seed(101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7-B7I7eMfLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating my model variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-hULL91MfLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use entropy = no limit on samples for split\n",
        "model_ent = DecisionTreeClassifier(criterion='entropy', random_state=1).fit(X_train, y_train) \n",
        "y_ent_pred = model_ent.predict(X_test)\n",
        "\n",
        "# Use information gain (default) limit min_samples to 4\n",
        "model_gini = DecisionTreeClassifier(min_samples_leaf=3).fit(X_train, y_train)\n",
        "y_gini_pred = model_gini.predict(X_test)\n",
        "\n",
        "# NOTE: You should, when testing models, only vary 1 thing at a time. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdXUBSqWMfLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a97ae1af-5f73-4e40-acde-8ed1c1399bcf"
      },
      "source": [
        "# Generate an accuracy Score\n",
        "print(\"Entropy accuracy is : {}%\".format(accuracy_score(y_test, y_ent_pred)*100))\n",
        "print(\"Gini accuracy is : {}%\".format(accuracy_score(y_test, y_gini_pred)*100))\n",
        "\n",
        "accuracy_ent = round(np.sum(accuracy_score(y_test, y_ent_pred)*100))\n",
        "accuracy_gini = round(np.sum(accuracy_score(y_test, y_gini_pred)*100))\n",
        "\n",
        "Accuracy_DecTree = pd.DataFrame(np.array([['DecTree_entropy',accuracy_ent], \n",
        "                                ['DecTree_gini',accuracy_gini]]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy accuracy is : 91.64556962025317%\n",
            "Gini accuracy is : 91.64556962025317%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tClvNSq9MfLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "a4a3e163-b4ea-4fe3-98a9-2b3ef08f5b8b"
      },
      "source": [
        "#confusion matrix\n",
        "pd.DataFrame(\n",
        "    confusion_matrix(y_test, y_gini_pred),\n",
        "    columns=['Predicted 1', 'Predicted -1'],\n",
        "    index=['True 1', 'True -1']\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted 1</th>\n",
              "      <th>Predicted -1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True 1</th>\n",
              "      <td>176</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True -1</th>\n",
              "      <td>19</td>\n",
              "      <td>186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Predicted 1  Predicted -1\n",
              "True 1           176            14\n",
              "True -1           19           186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-6HOnheMfLw",
        "colab_type": "text"
      },
      "source": [
        "Notice how many false predictions this model returns.\n",
        "\n",
        "#### Ensemble Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BvDwd7dMfLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bc17af54-179e-4381-865e-0395d1e89019"
      },
      "source": [
        "nTrees = 100\n",
        "max_depth = 10\n",
        "min_node_size = 10\n",
        "verbose = 0\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=nTrees, max_depth=max_depth, verbose=verbose, min_samples_leaf=min_node_size)\n",
        "clf.fit(X_train, y_train)\n",
        "print(clf.feature_importances_)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.05487941 0.05357306 0.03974006 0.28519772 0.23081786 0.04299097\n",
            " 0.0213586  0.2302777  0.02473168 0.01643293]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDI1y6n2MfL0",
        "colab_type": "text"
      },
      "source": [
        "From the feature importances above, it looks like our Random Forest classifier determined that the 3rd and 5th features in Wrapper_Features are the most useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pFw69YMfL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97985f1c-cf98-4bc9-bc77-658793e09d1e"
      },
      "source": [
        "y_rforest_pred = clf.predict(X_test)\n",
        "Accuracy = [1 for i in range(len(y_rforest_pred)) if y_test[i] == y_rforest_pred[i]]\n",
        "Accuracy = round(float(np.sum(Accuracy))/len(y_rforest_pred)*100,2)\n",
        "print(\"Accuracy on Testing Data = %.2f%%\"%Accuracy)\n",
        "\n",
        "Accuracy_ensemble = pd.DataFrame(np.array([['RandomForest',Accuracy]]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on Testing Data = 93.16%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rXY9BrNMfL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "3cc9d40f-66f0-49e8-e64a-5682c4205c41"
      },
      "source": [
        "#confusion matrix\n",
        "pd.DataFrame(\n",
        "    confusion_matrix(y_test, y_rforest_pred),\n",
        "    columns=['Predicted 1', 'Predicted -1'],\n",
        "    index=['True 1', 'True -1']\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted 1</th>\n",
              "      <th>Predicted -1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True 1</th>\n",
              "      <td>185</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True -1</th>\n",
              "      <td>22</td>\n",
              "      <td>183</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Predicted 1  Predicted -1\n",
              "True 1           185             5\n",
              "True -1           22           183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_otpzNzMfL-",
        "colab_type": "text"
      },
      "source": [
        "#### SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imkTJPhFMfL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cost = .9 # penalty parameter of the error term\n",
        "gamma = 5 # defines the influence of input vectors on the margins"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-lVCu7xMfMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining X as the feature columns but zscored\n",
        "X = pd.DataFrame(ss.zscore(X),columns=X.columns)\n",
        "\n",
        "#creating my model variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNJ6Vte8MfMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "7af72d27-0e12-409e-ee02-a6d028763397"
      },
      "source": [
        "# Test a LinearSVC\n",
        "clf1 = svm.LinearSVC(C=cost).fit(X_train, y_train)\n",
        "clf1.predict(X_test)\n",
        "print(\"LinearSVC\")\n",
        "print(classification_report(clf1.predict(X_test), y_test))\n",
        "\n",
        "Accuracy_svm = []\n",
        "\n",
        "# Test linear, rbf and poly kernels\n",
        "for k in ('linear', 'rbf', 'poly'):\n",
        "    clf = svm.SVC(gamma=gamma, kernel=k, C=cost).fit(X_train, y_train)\n",
        "    y_svm_pred = clf.predict(X_test)\n",
        "    Accuracy = [1 for i in range(len(y_svm_pred)) if y_test.iloc[i] == y_svm_pred[i]]\n",
        "    Accuracy = round(float(np.sum(Accuracy))/len(y_svm_pred)*100,2)\n",
        "    Accuracy_svm.append([k,Accuracy])\n",
        "    print(\"Accuracy on Testing Data = %.2f%%\"%Accuracy)    \n",
        "    print(k)\n",
        "    print(classification_report(clf.predict(X_test), y_test))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearSVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.62      0.65      0.64       193\n",
            "           1       0.65      0.62      0.63       202\n",
            "\n",
            "    accuracy                           0.64       395\n",
            "   macro avg       0.64      0.64      0.64       395\n",
            "weighted avg       0.64      0.64      0.64       395\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-4779e0bed7ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my_svm_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_svm_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_svm_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_svm_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mAccuracy_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-4779e0bed7ed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my_svm_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_svm_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_svm_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_svm_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mAccuracy_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLhT7b14MfMJ",
        "colab_type": "text"
      },
      "source": [
        "Notice how the SVM models didn't do as well at all even across many kernels. However, RBF seemed to do the best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQyzOleQMfMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accuracy_svm = pd.DataFrame(np.array([['SVM_linear', 71.03], ['SVM_rbf', 81.61], ['SVM_poly', 76.83]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO-w1SSjMfMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "f89f601a-d3a9-40eb-8314-4e5bade86e7f"
      },
      "source": [
        "# To make plotting easier, let's just use two features.\n",
        "X = X.iloc[:, :2]\n",
        "\n",
        "h = .5  # step size in the mesh\n",
        "#cost = .9  # update the cost\n",
        "#gamma = 10 # update the gamma \n",
        "\n",
        "# testing other kernels on unscaled data (for plotting tht support vectors)\n",
        "svc = svm.SVC(kernel='linear', C=cost).fit(X, Y)\n",
        "rbf_svc = svm.SVC(kernel='rbf', gamma=gamma, C=cost).fit(X, Y)\n",
        "poly_svc = svm.SVC(kernel='poly', gamma=gamma, degree=3, C=cost).fit(X, Y)\n",
        "lin_svc = svm.LinearSVC(C=cost).fit(X, Y)\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X.iloc[:,0].min() - 1, X.iloc[:,0].max() + 1\n",
        "y_min, y_max = Y.min() - 1, Y.max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# title for the plots\n",
        "titles = ['SVC with linear kernel',\n",
        "          'SVC with RBF kernel',\n",
        "          'SVC with polynomial kernel',\n",
        "          'LinearSVC (linear kernel)']\n",
        "\n",
        "for i, kernel in enumerate((svc, rbf_svc, poly_svc, lin_svc)):\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    Z = kernel.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    \n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Plot also the training points\n",
        "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=Y, cmap=plt.cm.Paired)\n",
        "    plt.title(titles[i])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0rpiizyMfMP",
        "colab_type": "text"
      },
      "source": [
        "What a weird shape! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRIRI7JEMfMQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "48905ce7-ccc2-43f3-fe62-0648f4bff407"
      },
      "source": [
        "#Final model comparison in terms of Accuracy\n",
        "Accuracy_df = pd.concat([Accuracy_DecTree,Accuracy_ensemble,Accuracy_svm])\n",
        "Accuracy_df.columns=['Model','Accuracy']\n",
        "Accuracy_df.sort_values(by='Accuracy',ascending=False)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>93.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DecTree_entropy</td>\n",
              "      <td>92.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DecTree_gini</td>\n",
              "      <td>92.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SVM_rbf</td>\n",
              "      <td>81.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SVM_poly</td>\n",
              "      <td>76.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SVM_linear</td>\n",
              "      <td>71.03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Model Accuracy\n",
              "0     RandomForest    93.16\n",
              "0  DecTree_entropy     92.0\n",
              "1     DecTree_gini     92.0\n",
              "1          SVM_rbf    81.61\n",
              "2         SVM_poly    76.83\n",
              "0       SVM_linear    71.03"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRr1JLIbMfMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "ef0ee137-c3ae-4388-9085-ad3815a7db97"
      },
      "source": [
        "ax = plt.figure(figsize=(15, 5)).gca()\n",
        "\n",
        "ax.scatter(Accuracy_df['Model'], Accuracy_df['Accuracy'])\n",
        "ax.set_ylim(ax.get_ylim()[::-1])\n",
        "ax.set_xlabel('Accuracy Mean')\n",
        "ax.set_ylabel('Model')\n",
        "ax.set_title('Showing Accuracy per Method')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Showing Accuracy per Method')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHRndyMdMfMU",
        "colab_type": "text"
      },
      "source": [
        "# Begin Milestone03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aNyH58TMfMV",
        "colab_type": "text"
      },
      "source": [
        "## simple NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoH5x4MpMfMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8f6cfd5-7618-481d-b7e7-eaf0ac880b86"
      },
      "source": [
        "X_resam.shape, Y_resam.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1972, 589), (1972,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaMow169MfMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the features, X, and the target is still Y from before\n",
        "X = X_resam[Wrapper_Features] \n",
        "Y = Y_resam\n",
        "\n",
        "#ensure that the decision tree is deterministic\n",
        "import numpy as np\n",
        "np.random.seed(101)\n",
        "\n",
        "#creating my model variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3CydLS4MfMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a numerically stable logistic s-shaped definition to call\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -500, 500)\n",
        "    if x.any()>=0:\n",
        "        return 1/(1 + np.exp(-x))\n",
        "    else:\n",
        "        return np.exp(x)/(1 + np.exp(x))\n",
        "   \n",
        "\n",
        "# define the dimentions and set the weights to random numbers\n",
        "def init_parameters(dim1, dim2=1,std=1e-1, random = True):\n",
        "    if(random):\n",
        "        return(np.random.random([dim1,dim2])*std)\n",
        "    else:\n",
        "        return(np.zeros([dim1,dim2]))\n",
        "   \n",
        "\n",
        "# Single layer network: Forward Prop\n",
        "# Passed in the weight vectors, bias vector, the input vector and the Y\n",
        "def fwd_prop(W1,bias,X,Y):\n",
        "\n",
        "    Z1 = np.dot(W1,X) + bias # dot product of the weights and X + bias\n",
        "    A1 = sigmoid(Z1)  # Uses sigmoid to create a predicted vector\n",
        "\n",
        "    return(A1)\n",
        "\n",
        "\n",
        "#Single layer network: Backprop\n",
        "def back_prop(A1,W1,bias,X,Y):\n",
        "\n",
        "    m = X.shape[1] # used the calculate the cost by the number of inputs -1/m\n",
        "   \n",
        "    # Cross entropy loss function\n",
        "    cost = (-1/m)*np.sum(Y*np.log(A1) + (1-Y)*np.log(1-A1)) # cost of error\n",
        "    dZ1 = A1 - Y                                            # subtract actual from pred weights\n",
        "    dW1 = (1/m) * np.dot(dZ1, X.T)                          # calc new weight vector\n",
        "    dBias = (1/m) * np.sum(dZ1, axis = 1, keepdims = True)  # calc new bias vector\n",
        "    \n",
        "    grads ={\"dW1\": dW1, \"dB1\":dBias} # Weight and bias vectors after backprop\n",
        "    \n",
        "    return(grads,cost)\n",
        "\n",
        "\n",
        "def run_grad_desc(num_epochs,learning_rate,X,Y,n_1):\n",
        "    \n",
        "    n_0, m = X.shape\n",
        "    \n",
        "    W1 = init_parameters(n_1, n_0, True)\n",
        "    B1 = init_parameters(n_1,1, True)\n",
        "    \n",
        "    loss_array = np.ones([num_epochs])*np.nan # resets the loss_array to NaNs\n",
        "    \n",
        "    for i in np.arange(num_epochs):\n",
        "        A1 = fwd_prop(W1,B1,X,Y)                # get predicted vector\n",
        "        grads,cost = back_prop(A1,W1,B1,X,Y)    # get gradient and the cost from BP \n",
        "        \n",
        "        W1 = W1 - learning_rate*grads[\"dW1\"]    # update weight vector LR*gradient*[BP weights]\n",
        "        B1 = B1 - learning_rate*grads[\"dB1\"]    # update bias LR*gradient[BP bias]\n",
        "        \n",
        "        loss_array[i] = cost                    # loss array gets cross ent values\n",
        "        \n",
        "        parameter = {\"W1\":W1,\"B1\":B1}           # assign \n",
        "    \n",
        "    return(parameter,loss_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C6EUph_MfMe",
        "colab_type": "text"
      },
      "source": [
        "## DNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr3RIQLtMfMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCILVa9IPk8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp5WgEq1QQk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "8c98fcc6-951e-425a-d031-9bb247bba8b9"
      },
      "source": [
        "# Create the model using keras Sequential api\n",
        "\n",
        "x_test = X_test.to_numpy()\n",
        "x_train = X_train.to_numpy()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_dim=10, activation='tanh', kernel_regularizer=l2(0.1)))\n",
        "model.add(Dropout(0.3, noise_shape=None , seed=None))\n",
        "\n",
        "model.add(Dense(100, activation='tanh', kernel_regularizer=l2(0.1)))\n",
        "model.add(Dropout(0.3, noise_shape=None , seed=None))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_61 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 11,301\n",
            "Trainable params: 11,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSy3IKwWi81M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f5255eb9-c068-4cfb-be61-281ee9e973aa"
      },
      "source": [
        "x_test.shape, y_test.sh"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((395, 10), (395,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn2zkLFfc2KR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrryneJrdBN5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f8dcc55-9733-41b0-c170-51df206db7a6"
      },
      "source": [
        "model_output = model.fit(x_train, y_train, epochs=100, batch_size=25, verbose=1, validation_data=(x_test, y_test))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1577 samples, validate on 395 samples\n",
            "Epoch 1/100\n",
            "1577/1577 [==============================] - 0s 198us/step - loss: 8.4963 - accuracy: 0.4724 - val_loss: 5.3125 - val_accuracy: 0.5063\n",
            "Epoch 2/100\n",
            "1577/1577 [==============================] - 0s 86us/step - loss: 3.4945 - accuracy: 0.4819 - val_loss: 2.1697 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "1577/1577 [==============================] - 0s 82us/step - loss: 1.6724 - accuracy: 0.4889 - val_loss: 1.0537 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "1577/1577 [==============================] - 0s 88us/step - loss: 1.0491 - accuracy: 0.5130 - val_loss: 0.8359 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/100\n",
            "1577/1577 [==============================] - 0s 82us/step - loss: 0.8329 - accuracy: 0.4775 - val_loss: 0.6655 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "1577/1577 [==============================] - 0s 88us/step - loss: 0.7598 - accuracy: 0.4788 - val_loss: 0.7378 - val_accuracy: 0.5063\n",
            "Epoch 7/100\n",
            "1577/1577 [==============================] - 0s 86us/step - loss: 0.7298 - accuracy: 0.4870 - val_loss: 0.7100 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/100\n",
            "1577/1577 [==============================] - 0s 87us/step - loss: 0.7177 - accuracy: 0.4965 - val_loss: 0.5664 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/100\n",
            "1577/1577 [==============================] - 0s 103us/step - loss: 0.7122 - accuracy: 0.5086 - val_loss: 0.6708 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/100\n",
            "1577/1577 [==============================] - 0s 93us/step - loss: 0.7082 - accuracy: 0.5060 - val_loss: 0.7280 - val_accuracy: 0.5063\n",
            "Epoch 11/100\n",
            "1577/1577 [==============================] - 0s 102us/step - loss: 0.7067 - accuracy: 0.4743 - val_loss: 0.7110 - val_accuracy: 0.5063\n",
            "Epoch 12/100\n",
            "1577/1577 [==============================] - 0s 86us/step - loss: 0.7040 - accuracy: 0.4902 - val_loss: 0.7482 - val_accuracy: 0.5063\n",
            "Epoch 13/100\n",
            "1577/1577 [==============================] - 0s 87us/step - loss: 0.7023 - accuracy: 0.5016 - val_loss: 0.7766 - val_accuracy: 0.5063\n",
            "Epoch 14/100\n",
            "1577/1577 [==============================] - 0s 91us/step - loss: 0.7029 - accuracy: 0.4933 - val_loss: 0.7406 - val_accuracy: 0.5063\n",
            "Epoch 15/100\n",
            "1577/1577 [==============================] - 0s 86us/step - loss: 0.7003 - accuracy: 0.4921 - val_loss: 0.7176 - val_accuracy: 0.5063\n",
            "Epoch 16/100\n",
            "1577/1577 [==============================] - 0s 90us/step - loss: 0.6995 - accuracy: 0.4997 - val_loss: 0.6544 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/100\n",
            "1577/1577 [==============================] - 0s 92us/step - loss: 0.7019 - accuracy: 0.4902 - val_loss: 0.6584 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/100\n",
            "1577/1577 [==============================] - 0s 81us/step - loss: 0.6995 - accuracy: 0.5162 - val_loss: 0.7340 - val_accuracy: 0.5063\n",
            "Epoch 19/100\n",
            "1577/1577 [==============================] - 0s 83us/step - loss: 0.7007 - accuracy: 0.4914 - val_loss: 0.6733 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "1577/1577 [==============================] - 0s 83us/step - loss: 0.7021 - accuracy: 0.4952 - val_loss: 0.6581 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/100\n",
            "1577/1577 [==============================] - 0s 82us/step - loss: 0.6994 - accuracy: 0.5086 - val_loss: 0.6069 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/100\n",
            "1577/1577 [==============================] - 0s 86us/step - loss: 0.6991 - accuracy: 0.5029 - val_loss: 0.6981 - val_accuracy: 0.5063\n",
            "Epoch 23/100\n",
            "1577/1577 [==============================] - 0s 85us/step - loss: 0.6983 - accuracy: 0.4997 - val_loss: 0.7311 - val_accuracy: 0.5063\n",
            "Epoch 24/100\n",
            "1577/1577 [==============================] - 0s 99us/step - loss: 0.6962 - accuracy: 0.4921 - val_loss: 0.7035 - val_accuracy: 0.5063\n",
            "Epoch 25/100\n",
            "1577/1577 [==============================] - 0s 88us/step - loss: 0.6954 - accuracy: 0.4870 - val_loss: 0.6585 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/100\n",
            "1577/1577 [==============================] - 0s 89us/step - loss: 0.6944 - accuracy: 0.4959 - val_loss: 0.6868 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/100\n",
            "1577/1577 [==============================] - 0s 85us/step - loss: 0.6943 - accuracy: 0.5086 - val_loss: 0.7120 - val_accuracy: 0.5063\n",
            "Epoch 28/100\n",
            "1577/1577 [==============================] - 0s 74us/step - loss: 0.6970 - accuracy: 0.5067 - val_loss: 0.8157 - val_accuracy: 0.5063\n",
            "Epoch 29/100\n",
            "1577/1577 [==============================] - 0s 75us/step - loss: 0.6990 - accuracy: 0.4990 - val_loss: 0.6255 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/100\n",
            "1577/1577 [==============================] - 0s 81us/step - loss: 0.7019 - accuracy: 0.5282 - val_loss: 0.6538 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/100\n",
            "1577/1577 [==============================] - 0s 83us/step - loss: 0.7036 - accuracy: 0.5111 - val_loss: 0.7018 - val_accuracy: 0.5063\n",
            "Epoch 32/100\n",
            "1577/1577 [==============================] - 0s 77us/step - loss: 0.6971 - accuracy: 0.5130 - val_loss: 0.6396 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/100\n",
            "1577/1577 [==============================] - 0s 77us/step - loss: 0.6956 - accuracy: 0.4952 - val_loss: 0.6468 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/100\n",
            "1577/1577 [==============================] - 0s 76us/step - loss: 0.6963 - accuracy: 0.4965 - val_loss: 0.7587 - val_accuracy: 0.5063\n",
            "Epoch 35/100\n",
            "1577/1577 [==============================] - 0s 83us/step - loss: 0.6965 - accuracy: 0.4883 - val_loss: 0.7704 - val_accuracy: 0.5063\n",
            "Epoch 36/100\n",
            "1577/1577 [==============================] - 0s 84us/step - loss: 0.6967 - accuracy: 0.4800 - val_loss: 0.6855 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "1577/1577 [==============================] - 0s 75us/step - loss: 0.6957 - accuracy: 0.4800 - val_loss: 0.6891 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/100\n",
            "1577/1577 [==============================] - 0s 80us/step - loss: 0.6952 - accuracy: 0.5130 - val_loss: 0.7630 - val_accuracy: 0.5063\n",
            "Epoch 39/100\n",
            "1577/1577 [==============================] - 0s 87us/step - loss: 0.6953 - accuracy: 0.5060 - val_loss: 0.6320 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/100\n",
            "1577/1577 [==============================] - 0s 86us/step - loss: 0.6952 - accuracy: 0.4762 - val_loss: 0.6992 - val_accuracy: 0.5063\n",
            "Epoch 41/100\n",
            "1577/1577 [==============================] - 0s 79us/step - loss: 0.6941 - accuracy: 0.4940 - val_loss: 0.7152 - val_accuracy: 0.5063\n",
            "Epoch 42/100\n",
            "1577/1577 [==============================] - 0s 85us/step - loss: 0.6954 - accuracy: 0.4731 - val_loss: 0.6955 - val_accuracy: 0.5063\n",
            "Epoch 43/100\n",
            "1577/1577 [==============================] - 0s 79us/step - loss: 0.6938 - accuracy: 0.4794 - val_loss: 0.7086 - val_accuracy: 0.5063\n",
            "Epoch 44/100\n",
            "1577/1577 [==============================] - 0s 85us/step - loss: 0.6939 - accuracy: 0.4889 - val_loss: 0.6990 - val_accuracy: 0.5063\n",
            "Epoch 45/100\n",
            "1577/1577 [==============================] - 0s 83us/step - loss: 0.6935 - accuracy: 0.4952 - val_loss: 0.7003 - val_accuracy: 0.5063\n",
            "Epoch 46/100\n",
            "1577/1577 [==============================] - 0s 81us/step - loss: 0.6942 - accuracy: 0.4864 - val_loss: 0.6862 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "1577/1577 [==============================] - 0s 89us/step - loss: 0.6946 - accuracy: 0.4940 - val_loss: 0.6731 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/100\n",
            "1577/1577 [==============================] - 0s 88us/step - loss: 0.6939 - accuracy: 0.5105 - val_loss: 0.7336 - val_accuracy: 0.5063\n",
            "Epoch 49/100\n",
            "1577/1577 [==============================] - 0s 77us/step - loss: 0.6964 - accuracy: 0.4876 - val_loss: 0.7120 - val_accuracy: 0.5063\n",
            "Epoch 50/100\n",
            "1577/1577 [==============================] - 0s 77us/step - loss: 0.6958 - accuracy: 0.5003 - val_loss: 0.6793 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/100\n",
            "1577/1577 [==============================] - 0s 81us/step - loss: 0.6950 - accuracy: 0.4952 - val_loss: 0.7105 - val_accuracy: 0.5063\n",
            "Epoch 52/100\n",
            "1577/1577 [==============================] - 0s 84us/step - loss: 0.6944 - accuracy: 0.4927 - val_loss: 0.6869 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/100\n",
            "1577/1577 [==============================] - 0s 88us/step - loss: 0.6940 - accuracy: 0.4997 - val_loss: 0.6913 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/100\n",
            "1577/1577 [==============================] - 0s 93us/step - loss: 0.6938 - accuracy: 0.4940 - val_loss: 0.6989 - val_accuracy: 0.5063\n",
            "Epoch 55/100\n",
            "1577/1577 [==============================] - 0s 88us/step - loss: 0.6944 - accuracy: 0.5067 - val_loss: 0.7015 - val_accuracy: 0.5063\n",
            "Epoch 56/100\n",
            "1577/1577 [==============================] - 0s 83us/step - loss: 0.6946 - accuracy: 0.5054 - val_loss: 0.6821 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/100\n",
            "1577/1577 [==============================] - 0s 76us/step - loss: 0.6949 - accuracy: 0.4807 - val_loss: 0.6799 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/100\n",
            "1577/1577 [==============================] - 0s 82us/step - loss: 0.6945 - accuracy: 0.4876 - val_loss: 0.6846 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/100\n",
            "1577/1577 [==============================] - 0s 82us/step - loss: 0.6940 - accuracy: 0.4883 - val_loss: 0.6853 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/100\n",
            "1577/1577 [==============================] - 0s 73us/step - loss: 0.6935 - accuracy: 0.4959 - val_loss: 0.6809 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/100\n",
            "1577/1577 [==============================] - 0s 75us/step - loss: 0.6934 - accuracy: 0.5016 - val_loss: 0.6824 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/100\n",
            "1577/1577 [==============================] - 0s 85us/step - loss: 0.6935 - accuracy: 0.4990 - val_loss: 0.6891 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/100\n",
            "1577/1577 [==============================] - 0s 79us/step - loss: 0.6940 - accuracy: 0.5143 - val_loss: 0.7287 - val_accuracy: 0.5063\n",
            "Epoch 64/100\n",
            "1577/1577 [==============================] - 0s 81us/step - loss: 0.6946 - accuracy: 0.5073 - val_loss: 0.6787 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/100\n",
            "1577/1577 [==============================] - 0s 81us/step - loss: 0.6941 - accuracy: 0.4990 - val_loss: 0.6937 - val_accuracy: 0.5063\n",
            "Epoch 66/100\n",
            "1577/1577 [==============================] - 0s 80us/step - loss: 0.6935 - accuracy: 0.4946 - val_loss: 0.6859 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/100\n",
            "1577/1577 [==============================] - 0s 90us/step - loss: 0.6937 - accuracy: 0.4889 - val_loss: 0.6842 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/100\n",
            "1577/1577 [==============================] - 0s 89us/step - loss: 0.6941 - accuracy: 0.4838 - val_loss: 0.6935 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/100\n",
            "1577/1577 [==============================] - 0s 84us/step - loss: 0.6938 - accuracy: 0.4813 - val_loss: 0.6983 - val_accuracy: 0.5063\n",
            "Epoch 70/100\n",
            "1577/1577 [==============================] - 0s 88us/step - loss: 0.6941 - accuracy: 0.4813 - val_loss: 0.6912 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/100\n",
            "1577/1577 [==============================] - 0s 76us/step - loss: 0.6941 - accuracy: 0.4800 - val_loss: 0.6938 - val_accuracy: 0.5063\n",
            "Epoch 72/100\n",
            "1577/1577 [==============================] - 0s 83us/step - loss: 0.6934 - accuracy: 0.4838 - val_loss: 0.6964 - val_accuracy: 0.5063\n",
            "Epoch 73/100\n",
            "1577/1577 [==============================] - 0s 76us/step - loss: 0.6933 - accuracy: 0.4902 - val_loss: 0.6955 - val_accuracy: 0.5063\n",
            "Epoch 74/100\n",
            "1577/1577 [==============================] - 0s 89us/step - loss: 0.6932 - accuracy: 0.4984 - val_loss: 0.6937 - val_accuracy: 0.5063\n",
            "Epoch 75/100\n",
            "1577/1577 [==============================] - 0s 81us/step - loss: 0.6932 - accuracy: 0.4870 - val_loss: 0.6940 - val_accuracy: 0.5063\n",
            "Epoch 76/100\n",
            "1577/1577 [==============================] - 0s 76us/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6896 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/100\n",
            "1577/1577 [==============================] - 0s 105us/step - loss: 0.6932 - accuracy: 0.5022 - val_loss: 0.6901 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/100\n",
            "1577/1577 [==============================] - 0s 86us/step - loss: 0.6932 - accuracy: 0.5048 - val_loss: 0.6874 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/100\n",
            "1577/1577 [==============================] - 0s 74us/step - loss: 0.6932 - accuracy: 0.4895 - val_loss: 0.6893 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/100\n",
            "1577/1577 [==============================] - 0s 82us/step - loss: 0.6932 - accuracy: 0.4952 - val_loss: 0.6897 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/100\n",
            "1577/1577 [==============================] - 0s 75us/step - loss: 0.6932 - accuracy: 0.5054 - val_loss: 0.6854 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "1577/1577 [==============================] - 0s 82us/step - loss: 0.6934 - accuracy: 0.5010 - val_loss: 0.6830 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "1577/1577 [==============================] - 0s 83us/step - loss: 0.6938 - accuracy: 0.5029 - val_loss: 0.7025 - val_accuracy: 0.5063\n",
            "Epoch 84/100\n",
            "1577/1577 [==============================] - 0s 75us/step - loss: 0.6941 - accuracy: 0.4965 - val_loss: 0.6864 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/100\n",
            "1577/1577 [==============================] - 0s 88us/step - loss: 0.6940 - accuracy: 0.4889 - val_loss: 0.6881 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/100\n",
            "1577/1577 [==============================] - 0s 85us/step - loss: 0.6935 - accuracy: 0.4851 - val_loss: 0.6889 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "1577/1577 [==============================] - 0s 84us/step - loss: 0.6933 - accuracy: 0.4997 - val_loss: 0.6911 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/100\n",
            "1577/1577 [==============================] - 0s 77us/step - loss: 0.6933 - accuracy: 0.4864 - val_loss: 0.6902 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/100\n",
            "1577/1577 [==============================] - 0s 73us/step - loss: 0.6934 - accuracy: 0.4769 - val_loss: 0.6923 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/100\n",
            "1577/1577 [==============================] - 0s 81us/step - loss: 0.6933 - accuracy: 0.4908 - val_loss: 0.6949 - val_accuracy: 0.5063\n",
            "Epoch 91/100\n",
            "1577/1577 [==============================] - 0s 87us/step - loss: 0.6933 - accuracy: 0.4971 - val_loss: 0.6900 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/100\n",
            "1577/1577 [==============================] - 0s 76us/step - loss: 0.6933 - accuracy: 0.4990 - val_loss: 0.6887 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/100\n",
            "1577/1577 [==============================] - 0s 84us/step - loss: 0.6933 - accuracy: 0.5003 - val_loss: 0.6872 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/100\n",
            "1577/1577 [==============================] - 0s 77us/step - loss: 0.6935 - accuracy: 0.5346 - val_loss: 0.8023 - val_accuracy: 0.5063\n",
            "Epoch 95/100\n",
            "1577/1577 [==============================] - 0s 77us/step - loss: 0.6971 - accuracy: 0.5060 - val_loss: 0.7019 - val_accuracy: 0.5063\n",
            "Epoch 96/100\n",
            "1577/1577 [==============================] - 0s 77us/step - loss: 0.6940 - accuracy: 0.4800 - val_loss: 0.6876 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/100\n",
            "1577/1577 [==============================] - 0s 79us/step - loss: 0.6933 - accuracy: 0.5016 - val_loss: 0.6875 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/100\n",
            "1577/1577 [==============================] - 0s 88us/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6863 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/100\n",
            "1577/1577 [==============================] - 0s 76us/step - loss: 0.6933 - accuracy: 0.5016 - val_loss: 0.6884 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/100\n",
            "1577/1577 [==============================] - 0s 75us/step - loss: 0.6934 - accuracy: 0.4990 - val_loss: 0.6896 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmxX97yVe2Q0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "74b275b1-a801-4457-adb9-9dc88194dbc8"
      },
      "source": [
        "print('Training Accuracy: ',np.mean(model_output.history['accuracy']))\n",
        "print('Validation Accuracy: ',np.mean(model_output.history['val_accuracy']))"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.49535203\n",
            "Validation Accuracy:  0.20759493887424468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y2zrKu1f8pW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ead72cd-3225-4cb5-bede-a6302f97d7db"
      },
      "source": [
        "y_pred = model.predict(x_test)\n",
        "rounded = [round(x[0]) for x in y_pred]\n",
        "y_pred1 = np.array(rounded, dtype='int64')\n",
        "\n",
        "average_precision_score(y_test, y_pred1)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5063291139240507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    }
  ]
}